{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CPS600 - Python Programming for Finance \n",
    "###  \n",
    "<img src=\"https://www.syracuse.edu/wp-content/themes/g6-carbon/img/syracuse-university-seal.svg?ver=6.3.9\" style=\"width: 200px;\"/>\n",
    "\n",
    "# Web Scraping & Crawling\n",
    "\n",
    "###  October 18, 2018\n",
    "\n",
    "We'll look at some more tools for pulling data down off the web."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scraping**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For starters, we have mature tools for parsing HTML (XML broadly speaking). Here is **lxml**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The yoozhe\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we are going to grab some html\n",
    "import lxml.html as lh\n",
    "\n",
    "# Simply way to make HTTP requests\n",
    "import requests\n",
    "\n",
    "URL = \"https://en.wikipedia.org/wiki/HTML\"\n",
    "r = requests.get(URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we take the response and turn it into an *element tree*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree   = lh.fromstring(r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now get information by using the structure of the HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.findtext('head/title')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, we can explore the structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "childs = tree.xpath('child::*')\n",
    "childs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second child is the body of the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "body = childs[1]\n",
    "\n",
    "body.attrib['class']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look for all div elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "divList = body.xpath('div')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I wonder what is in them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "divList[0].values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And how many?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(divList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not that many, we must have meant *find all of them*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "allDivs = body.xpath('//div')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(allDivs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I wonder whether there are any interesting tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "allTables = body.xpath('//table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(allTables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.imgur.com/QQeZcZL.gif\" style=\"width: 200px;\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = allTables[0]\n",
    "table.values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is not much information. What else is in there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(table[1].xpath('tr'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So there is a table with many rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There aren't any structured datasets available in this page (just inspect). OK. Let's try another page from which we can pull some data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"https://en.wikipedia.org/wiki/Python_(programming_language)\"\n",
    "p = requests.get(URL)\n",
    "\n",
    "ptree = lh.fromstring(p.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the source, we know what we are looking for. We can select by attribute values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pTables = ptree.xpath(\"//table[@class='wikitable']\")\n",
    "len(pTables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what we've got here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart = pTables[0]\n",
    "chart.xpath('child::*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "lastBody = chart[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lastBody.xpath('tr'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** Let's build a `DataFrame` that represents the data in the table stored (and parsed as a tree) in `lastBody`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toNP = [[x.text_content().strip() for x in r.xpath('th')] for r in lastBody.xpath('tr') ]\n",
    "toNP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, that's not exactly what we expected, but it got the first row, so we can go ahead and use that for columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tableCols = toNP[0]\n",
    "tableCols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But how to get the remaining data? Let's redo that list comprehension. There is no way around it; we need a helper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tableHelper(node):\n",
    "    if len(node.xpath('code')) > 0:\n",
    "        codes = node.xpath('code')\n",
    "        txtCodes = ' '.join([x.text_content().strip() for x in codes])\n",
    "        return txtCodes\n",
    "    else:\n",
    "        return node.text_content().strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the changes below. We are lookin in different tags (`td`) because that is where the data are. We have inserted our helper function in order to process the nodes properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toNP = [[tableHelper(x) for x in r.xpath('td')] for r in lastBody.xpath('tr')[1:] ]\n",
    "toNP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this is imperfect, but did we at least get the basic structure right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(toNP), [len(x) for x in toNP], len([len(x) for x in toNP])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, everything has the right shape. Now let's build an `array` of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tableData = np.array(toNP)\n",
    "tableData.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good, now let's build `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tableFrame = pd.DataFrame(data=tableData, columns=tableCols)\n",
    "tableFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is of course, room for improvement. Generally, when engaged in a scraping endeavor you will be interested in a specific kind of element in a specific kind of web page and therefore you will specialize your processing steps in order to clean things up. This is necessary; no amount of 'wrapping' or elegance will free us from having to express what we mean. That said, let's do some wrapping and also make use of some pre-packaged wrapping!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, I am going to put everything into a single function `getFrame`. My function *takes a string* and it *returns a `DataFrame`*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copied from above for completeness of this cell.\n",
    "def tableHelper(node):\n",
    "    if len(node.xpath('code')) > 0:\n",
    "        codes = node.xpath('code')\n",
    "        txtCodes = ' '.join([x.text_content().strip() for x in codes])\n",
    "        return txtCodes\n",
    "    else:\n",
    "        return node.text_content().strip()\n",
    "\n",
    "def getFrame(name):\n",
    "    \"\"\" Give me your thing\n",
    "    that you want me to \n",
    "    Wikipedia and I will\n",
    "    try to extract the first\n",
    "    table from it.\"\"\"\n",
    "    # Request and results\n",
    "    base = \"https://en.wikipedia.org/wiki/\"\n",
    "    URL = base+name\n",
    "    r = requests.get(URL)\n",
    "    # Prepare my element tree structure\n",
    "    tree   = lh.fromstring(r.text)\n",
    "    \n",
    "    # Finding the table\n",
    "    pTables = tree.xpath(\"//table[@class='wikitable']\")\n",
    "    Body = pTables[0].xpath('tbody')[0]\n",
    "\n",
    "    # Extracting the data\n",
    "    toNP = [[tableHelper(x) for x in r.xpath('td')] for r in Body.xpath('tr')[1:] ]\n",
    "    tableData = np.array(toNP)\n",
    "    \n",
    "    # Extracting the column names\n",
    "    header = Body.xpath('tr')[0]\n",
    "    tableCols = [x.text_content().strip() for x in header.xpath('th')]\n",
    "    \n",
    "    # Building the DataFrame\n",
    "    tableFrame = pd.DataFrame(data=tableData, columns=tableCols)\n",
    "    \n",
    "    return tableFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newFrame = getFrame('porsche')\n",
    "newFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** (for the reader)\n",
    "1. Make this function more robust against errors/incompatible pages\n",
    "2. Make this function more fully-featured (w.r.t input)\n",
    "3. Make this function more refined (w.r.t output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at *Beautiful Soup*, named for *tag soup* - a term of endearment for messy HTML or XML. The `lxml` [documentation](https://lxml.de/) mentions Beautiful Soup as an alternative for handling *really broken* HTML documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example from the Wikipedia page on BS4\n",
    "# Note the use of the native urllib\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "\n",
    "with urllib.request.urlopen('https://en.wikipedia.org/wiki/Main_Page') as response:\n",
    "    webpage = response.read()\n",
    "    soup = BeautifulSoup(webpage, 'html.parser')\n",
    "    for anchor in soup.find_all('a'):\n",
    "        print(anchor.get('href', '/'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to replicate what we did above, this time using `bs4`. The setup is the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = \"https://en.wikipedia.org/wiki/\" \n",
    "URL = base+\"Python_(programming_language)\"\n",
    "r = requests.get(URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we parse the source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(r.text)\n",
    "table = soup.findAll('table',{'class':'wikitable'})[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually, the rest of this goes pretty much as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Illustration of the methods we're using\n",
    "header = table.findAll('th')[0]\n",
    "header.text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's recreate that list comprehension from before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "toNP = [[x.text.strip() for x in r.findAll('th')] for r in table.findAll('tr') ]\n",
    "tableCols = toNP[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should probably rewrite `tableHelper`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tableHelper(node):\n",
    "    if len(node.findAll('code')) > 0:\n",
    "        codes = node.findAll('code')\n",
    "        txtCodes = ' '.join([x.text.strip() for x in codes])\n",
    "        return txtCodes\n",
    "    else:\n",
    "        return node.text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "toNP = [[tableHelper(x) for x in r.findAll('td')] for r in table.findAll('tr')[1:] ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rest is as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tableData = np.array(toNP)\n",
    "tableFrame = pd.DataFrame(data=tableData, columns=tableCols)\n",
    "tableFrame.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a brief illustration of the options in parsing HTML responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = \"<a><b /></a>\"\n",
    "doc2 = \"<a></p>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `bs4` in a few different ways. Here we parse `doc1` as HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BeautifulSoup(doc1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we parse the same document as XML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BeautifulSoup(doc1,'xml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use the `lxml` library to parse the second, invalid document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BeautifulSoup(doc2, 'lxml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `lxml` library decided to simple drop the dangling end tag. What will the `html5lib` library do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BeautifulSoup(doc2,'html5lib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead, this one filled in the missing start tag."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**REGEX**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a brief look at *regular expressions*. This is another tool that will serve you well in wrangling real-world data, particularly text data. This discussion of scraping is as good a place as any for a review of regexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "tedURL = \"https://www.washingtonpost.com/wp-srv/national/longterm/unabomber/manifesto.text.htm?noredirect=on\"\n",
    "tedMan = requests.get(tedURL)\n",
    "tedParags =  [x.text_content().strip() for x in lh.fromstring(tedMan.text).xpath('//p')]\n",
    "tedText = 'PARAG'.join(tedParags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We didn't really need that last part, it was just for fun. Now we have a really long string. Let's look for patterns in it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">A regular expression (or RE) specifies a set of strings that matches it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the rest of the documentation [here](https://docs.python.org/3/library/re.html). Let's look at some examples. We inserted those \"PARAG\" strings. What follows them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example below uses a *lookahead* pattern - we don't extract the \"PARAG\", but the thing that follows it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "parags = re.findall('(?<=PARAG).*',tedText)\n",
    "parags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's pretty close to what I wanted. Note that most of these are paragraphs with the author's original numbering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parags[15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why does it just stop right there? From the documentation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> `.` matches any character except a newline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That explains it. So what if we wanted just the first word following a \"PARAG\"? We could do, for instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "parags = re.findall('(?<=PARAG)\\S*',tedText)\n",
    "parags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad. What about the next couple of words? (**exercise**). There is (almost) no limit to what you can express with REs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scrapy**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a tool for automating web scraping.\n",
    ">Scrapy is an application framework for crawling web sites and extracting structured data which can be used for a wide range of useful applications, like data mining, information processing or historical archival."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's step through the tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create a new terminal in Jupyter (or otherwise open a new terminal)\n",
    "2. Enter `scrapy startproject tutorial` (or another name for your tutorial project)\n",
    "3. Copy or type the below code into a text file that we'll call `quotes_spider.py`. Create that file in `tutorial/tutorial/spiders`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class definition for your first scrapy spider\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = \"quotes\"\n",
    "\n",
    "    def start_requests(self):\n",
    "        urls = [\n",
    "            'http://quotes.toscrape.com/page/1/',\n",
    "            'http://quotes.toscrape.com/page/2/',\n",
    "        ]\n",
    "        for url in urls:\n",
    "            yield scrapy.Request(url=url, callback=self.parse)\n",
    "\n",
    "    def parse(self, response):\n",
    "        page = response.url.split(\"/\")[-2]\n",
    "        filename = 'quotes-%s.html' % page\n",
    "        with open(filename, 'wb') as f:\n",
    "            f.write(response.body)\n",
    "        self.log('Saved file %s' % filename) # old-fashioned string parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remarks on these definitions:\n",
    "* `name` identifies the Spider. It must be unique within a project, that is, you can’t set the same name for different Spiders.\n",
    "\n",
    "* `start_requests()` must return an iterable of Requests (you can return a list of requests or write a generator function) which the Spider will begin to crawl from. Subsequent requests will be generated successively from these initial requests.\n",
    "\n",
    "* `parse()` a method that will be called to handle the response downloaded for each of the requests made. The response parameter is an instance of TextResponse that holds the page content and has further helpful methods to handle it.\n",
    "The `parse()` method usually parses the response, extracting the scraped data as dicts and also finding new URLs to follow and creating new requests (`Request`) from them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Go to the top-level directory (`tutorial`) and run `scrapy crawl quotes`.\n",
    "5. Look around (`ls`) the directory. Examine the new files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What you'll notice about this example is that we really didn't do any parsing. Let's fix that by updating our spider. Replace the parse method definition in `quotes_spider.py` with the new one below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(*Remark* you can use the scrapy shell to play with the methods used below: `scrapy shell \"some_url.com\"`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(self, response):\n",
    "    for quote in response.css('div.quote'):\n",
    "        yield {\n",
    "            'text': quote.css('span.text::text').extract_first(),\n",
    "            'author': quote.css('small.author::text').extract_first(),\n",
    "            'tags': quote.css('div.tags a.tag::text').extract(),\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, run the command `scrapy crawl quotes -o quotes.json` to extract data from these pages and store the parsed results in the file `quotes.json`.\n",
    "\n",
    "Then take a look (e.g. `cat quotes.json`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are sort of scraping, but there is no *crawling* to speak of. What does that mean, crawling? Let's see how to follow links."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace the contents of `quotes_spider.py` with the code below. Note that this one also uses the `start_urls` shortcut (which you can read about on the tutorial page)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = \"quotes\"\n",
    "    start_urls = [\n",
    "        'http://quotes.toscrape.com/page/1/',\n",
    "    ]\n",
    "\n",
    "    def parse(self, response):\n",
    "        for quote in response.css('div.quote'):\n",
    "            yield {\n",
    "                'text': quote.css('span.text::text').extract_first(),\n",
    "                'author': quote.css('small.author::text').extract_first(),\n",
    "                'tags': quote.css('div.tags a.tag::text').extract(),\n",
    "            }\n",
    "\n",
    "        next_page = response.css('li.next a::attr(href)').extract_first()\n",
    "        if next_page is not None:\n",
    "            next_page = response.urljoin(next_page)\n",
    "            yield scrapy.Request(next_page, callback=self.parse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice what has changed: we have extracted links and included logic to move to the next one in the list. More precisely...\n",
    ">Now, after extracting the data, the `parse()` method looks for the link to the next page, builds a full absolute URL using the `urljoin()` method (since the links can be relative) and yields a new request to the next page, registering itself as callback to handle the data extraction for the next page and to keep the crawling going through all the pages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What else can you do? Here is one example of a slightly more advanced spider that scrapes author information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "\n",
    "class AuthorSpider(scrapy.Spider):\n",
    "    name = 'author'\n",
    "\n",
    "    start_urls = ['http://quotes.toscrape.com/']\n",
    "\n",
    "    def parse(self, response):\n",
    "        # follow links to author pages\n",
    "        for href in response.css('.author + a::attr(href)'):\n",
    "            yield response.follow(href, self.parse_author)\n",
    "\n",
    "        # follow pagination links\n",
    "        for href in response.css('li.next a::attr(href)'):\n",
    "            yield response.follow(href, self.parse)\n",
    "\n",
    "    def parse_author(self, response):\n",
    "        def extract_with_css(query):\n",
    "            return response.css(query).extract_first().strip()\n",
    "\n",
    "        yield {\n",
    "            'name': extract_with_css('h3.author-title::text'),\n",
    "            'birthdate': extract_with_css('.author-born-date::text'),\n",
    "            'bio': extract_with_css('.author-description::text'),\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the tutorial:\n",
    ">This spider will start from the main page, it will follow all the links to the authors pages calling the `parse_author` callback for each of them, and also the pagination links with the `parse` callback as we saw before."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
