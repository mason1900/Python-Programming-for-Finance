{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CPS600 - Python Programming for Finance \n",
    "###  \n",
    "<img src=\"https://www.syracuse.edu/wp-content/themes/g6-carbon/img/syracuse-university-seal.svg?ver=6.3.9\" style=\"width: 200px;\"/>\n",
    "\n",
    "# Distributions, Likelihood & Bayesian Analysis\n",
    "\n",
    "###  October 16, 2018\n",
    "\n",
    "Much of today's content is gotten from the [SESYNC Workshop](https://www.sesync.org/opportunities/research-short-courses/bayesian-modeling-for-socio-environmental-data) on Bayesian modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last time we had a whirlwind of probability and statistics ideas. This time, we'll slow down and look at some interesting applications of Bayes' theorem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lab Review**\n",
    "\n",
    "After we see how to do maximum likelihood estimation, you will be *theoretically* equipped to do maximum likelihood estimation on the Weibull distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Loading the data\n",
    "df = pd.read_csv('distroCSV.csv')\n",
    "distro1 = df['distro1']\n",
    "distro2 = df['distro2']\n",
    "distro3 = df['distro3']\n",
    "distro4 = df['distro4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters of unknown1, normal distribution: sigma: 1.0264711046684125 mu: 0.9951602680401787\n",
      "parameters of unknown4, log normal distribution: sigma: 0.6166121027028655 mu: -0.48964165969425616\n",
      "parameters of unknown2, gamma distribution: k: 0.06820157375523678 theta: 368.71742057622646\n",
      "parameters of unknown3, weibull distribution: lam: 0.3484598061921611 k: 0.6263839326870151\n"
     ]
    }
   ],
   "source": [
    "# Thanks to Fangzhi for the 'solutions' below.\n",
    "# (Some edits are necessary - demonstration purposes only)\n",
    "import math\n",
    "import numpy as np\n",
    "from scipy.optimize import fsolve\n",
    "\n",
    "d1_std, d2_var, d3_std, d4_var = (np.std(np.array(distro1)),\n",
    "                               np.var(np.array(distro2)),\n",
    "                               np.std(np.array(distro3)),\n",
    "                               np.var(np.array(distro4)))\n",
    "                               \n",
    "d1_avg, d2_avg, d3_avg, d4_avg = (np.mean(np.array(distro1)),\n",
    "                                  np.mean(np.array(distro2)),\n",
    "                                  np.mean(np.array(distro3)),\n",
    "                                  np.mean(np.array(distro4)))\n",
    "\n",
    "print(\"parameters of unknown1, normal distribution:\",\"sigma:\",d1_std,\"mu:\",d1_avg)\n",
    "\n",
    "# For the log-normal\n",
    "def logNormal(x):\n",
    "    sigma,mu = x\n",
    "    return [math.exp(mu+sigma**2/2)-d4_avg, \n",
    "            (math.exp(sigma**2)-1)*math.exp(2*mu+sigma**2)-d4_var]\n",
    "result = fsolve(logNormal,[1,1])\n",
    "print(\"parameters of unknown4, log normal distribution:\",\"sigma:\",\n",
    "      result[0],\"mu:\",\n",
    "      result[1])\n",
    "\n",
    "\n",
    "#unknown2 gamma distriubtion,where E(x)= k * thetaï¼Œ variance = k * theta^2\n",
    "theta = d2_var/d2_avg\n",
    "k = d2_avg/theta\n",
    "print(\"parameters of unknown2, gamma distribution:\",\"k:\",k,\"theta:\",theta)\n",
    "\n",
    "#unknown 3 weibull distribution\n",
    "def w(x):\n",
    "    lam,k = x\n",
    "    return [lam * math.gamma(1+1/k)-d3_avg,\n",
    "            (lam**2)*(math.gamma(1+2/k)-(math.gamma(1+1/k))**2)-d3_std]\n",
    "result = fsolve(w, [1,1])\n",
    "print('parameters of unknown3, weibull distribution: lam:',result[0],'k:',result[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some good documentation [here](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.fsolve.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bayes' Theorem**\n",
    "\n",
    "Once again, before we dive in, recall what we learned last time about conditional probability and *Bayes' Theorem*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ P(B \\mid A) = \\frac{P(A \\mid B)P(B)}{P(A)} = \\frac{P(A \\mid B)P(B)}{\\sum_k P(A \\mid B_k)P(B_k)} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *likelihood* of a parameter (or vector of parameters) $p$ given some observations $X$ is defined to be the probability of observing the data given the parameter.\n",
    "\n",
    "$$ L(p \\mid X) = P(X \\mid p)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually, the definition is looser than that because we do not require that the likelihood be a probability perse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Likelihood**\n",
    "\n",
    "Why *likelihood*?\n",
    "* Likelihood is a component of all Bayesian models\n",
    "* Maximum likelihood is an important statistical approach in its own right.\n",
    "\n",
    "Our objectives here:\n",
    "* Understand the concept of likelihood and its relationship to the probability of data conditional on parameters\n",
    "* Describe a likelihood profile and how it differs from a probability density function\n",
    "* Understand how to compose likelihoods for multiple parameters and multiple observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a distinction between estimating data from a parameter vs estimating a parameter from data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"infLike.png\" style=\"width: 800px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:\n",
    "> The true proportion of defective units, $\\theta$, of widgets produced by a Syracuse factory is $.12$. A sample of $24$ units includes $4$ defective units. What is the probability of obtaining these data assuming the estimated proportion is true? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"inferFinal\n",
    ".png\" style=\"width: 800px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the other hand, *Bayesian inference* is based on the likelihood function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"BayesThetaPic.png\" style=\"width: 800px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:\n",
    "> We obtain a new sample of $24$ widgets from the same Syracuse factory and find that it includes $4$ defective units. In light of these data, what is the probability that the true value of prevalence, $\\theta$ , is found in\n",
    "$[q_L, q_U]$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key idea in likelihood is this\n",
    "\n",
    "* In a probability mass or probability density function, the parameter $\\theta$ is constant and the data $y$ are random variables. The function sums or integrates to $1$ over its support.\n",
    "\n",
    "* In a likelihood function, the data are constant (known) and the parameter is unknown but fixed. We use $[y\\mid \\theta]$ to assess the likelihood of different values of $\\theta$ in light of the data. In this case, the function does not sum or integrate to one over all possible values of the parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ L(\\theta \\mid y) \\propto [y \\mid \\theta] $$\n",
    "\n",
    "Likelihood is *proportional* to probability or probability density."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pause and discuss this notation a little.\n",
    "\n",
    "$$L(\\theta \\mid y) \\propto [y \\mid \\theta]$$\n",
    "$$L(\\theta \\mid y) = c[y \\mid \\theta]$$\n",
    "$$L(\\theta \\mid y) = [y \\mid \\theta]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's have some more concrete examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*when the parameter is known and the data are random*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"canBeans.png\" style=\"width: 800px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* What are the possible outcomes?\n",
    "* What PMF (probability mass function) would you use to model these data?\n",
    "* What is the probability of each outcome?\n",
    "* What is the sum of the individual probabilities?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:\n",
    "\n",
    "<img src=\"beansNumbs.png\" style=\"width: 800px;\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*when data are known and parameter is random*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"manyCans.png\" style=\"width: 800px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* What is the likelihood of each possible value of the parameters?\n",
    "* How do the values sum?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:\n",
    "\n",
    "> We have three hypothesized parameter values, $\\frac{5}{6}, \\frac{1}{2}, \\frac{1}{6}$. Data in hand are $2$ white beans on $3$ draws. The likelihood of each parameter value is given in the table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<img src=\"likelyTab.png\" style=\"width: 800px;\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize this in a plot (I will later ask you to generate such a plot)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*A Likelihood Profile: 2 White Beans on 3 Draws*\n",
    "\n",
    "<img src=\"likelyProfl.png\" style=\"width: 800px;\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class Exercise: Can of Beans\n",
    "* Let's draw $10$ beans from one of three cans\n",
    "* How much more likely is this can than the other two?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.binomial(10,1/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*A Likelihood Profile: 4 White Beans on 10 Draws*\n",
    "\n",
    "<img src=\"fourBeans.png\" style=\"width: 800px;\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things to notice here:\n",
    "* The shape of the curve has narrowed\n",
    "* The center of the curve has shifted\n",
    "* One of the possible values is much more likely than the others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary:\n",
    "\n",
    "*Likelihood vs Probability, Summarized*\n",
    "\n",
    "<img src=\"summary.png\" style=\"width: 900px;\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You're probably wondering about multiple parameters. We won't get into those computations, but let's look at how that is handled conceptually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used a single parameter, and that made it easy for us to visualize the problem. In general, we can have any number of parameters in a much more complex model of some process, e.g.\n",
    "\n",
    "$$  \\mu_i = g(\\theta, x_i) $$\n",
    "$$ L(\\mu_i \\mid y_i) \\propto [y_i \\mid \\mu_i]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going up one dimension, we can visualize this as a *likelihood surface*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"likelySurf.png\" style=\"width: 900px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we assume *conditional independence*, then the likelihood is the **product of the individual likelihoods**. This is a pretty big assumption, but you've got to start somewhere.\n",
    "\n",
    "$$ L(\\theta \\mid {\\bf y}) = c\\prod_{i=1}^n [y_i \\mid {\\bf \\theta}]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The consequences of conditional independence (for us here) are that the *chain rule*:\n",
    "\n",
    "$$ P(y_1,\\ldots,y_n) = P(y_1 \\mid y_2,\\ldots,y_n)P(y_2 \\mid y_3,\\ldots,y_n)\\ldots P(y_n)$$\n",
    "\n",
    "in this case implies that we can get a joint probability as a product:\n",
    "\n",
    "$$ P(y_1,\\ldots,y_n) = P(y_1)\\cdots P(y_n)$$\n",
    "\n",
    "(which is what we wanted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PMF [K|b] = (n  k) b^k (1-b)^(n-k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Theoretically, this is all we need. But *numerically* it is not feasible to go around multiplying zillions of probbailities together - you get precision problems and lose information rapidly. We therefore take the logs and sum them:\n",
    "\n",
    "$$ \\log L(\\theta \\mid {\\bf y}) = \\log c + \\sum_{i=1}^n \\log [y_i \\mid {\\bf \\theta}] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pop Quiz** Why can we get away with this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see a *continuous* example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: The Exponential Distribution & POTUS tweets\n",
    "\n",
    "Supported on the *positive semiaxis*, an exponential random variable is described by a single paramter $\\lambda$.\n",
    "\n",
    "$$ P(t_i \\mid \\lambda) = \\lambda e^{-\\lambda t_i} $$\n",
    "\n",
    "In a picture:\n",
    "\n",
    "<img src=\"expPlots.png\" style=\"width: 600px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The exponential distribution describes the 'waiting times' between events resulting from a [Poisson process](https://en.wikipedia.org/wiki/Poisson_point_process).\n",
    "\n",
    "The parameter $\\lambda$ is the average number of occurrences per unit time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at POTUS tweets.\n",
    "\n",
    "> Generate a datum quantifying how long we have to wait for a new tweet from POTUS. Write out the likelihood function *assuming wait times for POTUS tweets are governed by an exponential distribution*. Determine the maximum likelihood estimate (MLE) for $\\lambda$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's suppose we're looking to quantify the rate *per day* of tweets. From a certain sample we get:\n",
    "\n",
    "* $991 / 151 = 6.6$ tweets per day\n",
    "* $t_1 = 1 / 6.6 = 0.15$ days between tweets\n",
    "\n",
    "With this single datapoint, we can analytically solve for the maximum likelihood estimator $\\lambda_{MLE}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The definition\n",
    "\n",
    "$$ L(\\lambda \\mid t_1) = [t_1 \\mid \\lambda] $$\n",
    "\n",
    "The assumption on the distribution\n",
    "\n",
    "$$ L(\\lambda) = \\lambda e^{-\\lambda t_1} $$\n",
    "\n",
    "Taking logs\n",
    "\n",
    "$$ LL(\\lambda) = \\log(\\lambda) - \\lambda t_1 $$\n",
    "\n",
    "Differentiating...\n",
    "\n",
    "$$ \\frac{dLL}{ d\\lambda} = \\frac{1}{\\lambda} - t_1  $$\n",
    "\n",
    "Equating to zero\n",
    "\n",
    "$$ 0 = \\frac{1}{\\lambda_{MLE}} - t_1 $$\n",
    "\n",
    "Solving...\n",
    "\n",
    "$$ \\lambda_{MLE} = 6.6 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what does this look like? Here is the likelihood profile:\n",
    "\n",
    "<img src=\"potusProfl.png\" style=\"width: 800px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what about the density? We would like a *posterior distribution*, and not just the *likelihood profile*."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
