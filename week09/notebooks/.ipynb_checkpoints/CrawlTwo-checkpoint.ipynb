{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CPS600 - Python Programming for Finance \n",
    "###  \n",
    "<img src=\"https://www.syracuse.edu/wp-content/themes/g6-carbon/img/syracuse-university-seal.svg?ver=6.3.9\" style=\"width: 200px;\"/>\n",
    "\n",
    "## Scraping & Crawling, Cont'd\n",
    "\n",
    "###  October 23, 2018\n",
    "\n",
    "We'll look at some more tools for pulling data down off the web."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**REGEX**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a brief look at *regular expressions*. This is another tool that will serve you well in wrangling real-world data, particularly text data. This discussion of scraping is as good a place as any for a review of regexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "tedURL = \"https://www.washingtonpost.com/wp-srv/national/longterm/unabomber/manifesto.text.htm?noredirect=on\"\n",
    "tedMan = requests.get(tedURL)\n",
    "tedParags =  [x.text_content().strip() for x in lh.fromstring(tedMan.text).xpath('//p')]\n",
    "tedText = 'PARAG'.join(tedParags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We didn't really need that last part, it was just for fun. Now we have a really long string. Let's look for patterns in it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">A regular expression (or RE) specifies a set of strings that matches it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the rest of the documentation [here](https://docs.python.org/3/library/re.html). Let's look at some examples. We inserted those \"PARAG\" strings. What follows them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example below uses a *lookahead* pattern - we don't extract the \"PARAG\", but the thing that follows it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "parags = re.findall('(?<=PARAG).*',tedText)\n",
    "parags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's pretty close to what I wanted. Note that most of these are paragraphs with the author's original numbering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parags[15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why does it just stop right there? From the documentation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> `.` matches any character except a newline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That explains it. So what if we wanted just the first word following a \"PARAG\"? We could do, for instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "parags = re.findall('(?<=PARAG)\\S*',tedText)\n",
    "parags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad. What about the next couple of words? (**exercise**). There is (almost) no limit to what you can express with REs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scrapy**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a tool for automating web scraping.\n",
    ">Scrapy is an application framework for crawling web sites and extracting structured data which can be used for a wide range of useful applications, like data mining, information processing or historical archival."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's step through the tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create a new terminal in Jupyter (or otherwise open a new terminal)\n",
    "2. Enter `scrapy startproject tutorial` (or another name for your tutorial project)\n",
    "3. Copy or type the below code into a text file that we'll call `quotes_spider.py`. Create that file in `tutorial/tutorial/spiders`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class definition for your first scrapy spider\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = \"quotes\"\n",
    "\n",
    "    def start_requests(self):\n",
    "        urls = [\n",
    "            'http://quotes.toscrape.com/page/1/',\n",
    "            'http://quotes.toscrape.com/page/2/',\n",
    "        ]\n",
    "        for url in urls:\n",
    "            yield scrapy.Request(url=url, callback=self.parse)\n",
    "\n",
    "    def parse(self, response):\n",
    "        page = response.url.split(\"/\")[-2]\n",
    "        filename = 'quotes-%s.html' % page\n",
    "        with open(filename, 'wb') as f:\n",
    "            f.write(response.body)\n",
    "        self.log('Saved file %s' % filename) # old-fashioned string parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remarks on these definitions:\n",
    "* `name` identifies the Spider. It must be unique within a project, that is, you canâ€™t set the same name for different Spiders.\n",
    "\n",
    "* `start_requests()` must return an iterable of Requests (you can return a list of requests or write a generator function) which the Spider will begin to crawl from. Subsequent requests will be generated successively from these initial requests.\n",
    "\n",
    "* `parse()` a method that will be called to handle the response downloaded for each of the requests made. The response parameter is an instance of TextResponse that holds the page content and has further helpful methods to handle it.\n",
    "The `parse()` method usually parses the response, extracting the scraped data as dicts and also finding new URLs to follow and creating new requests (`Request`) from them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Go to the top-level directory (`tutorial`) and run `scrapy crawl quotes`.\n",
    "5. Look around (`ls`) the directory. Examine the new files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What you'll notice about this example is that we really didn't do any parsing. Let's fix that by updating our spider. Replace the parse method definition in `quotes_spider.py` with the new one below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(*Remark* you can use the scrapy shell to play with the methods used below: `scrapy shell \"some_url.com\"`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(self, response):\n",
    "    for quote in response.css('div.quote'):\n",
    "        yield {\n",
    "            'text': quote.css('span.text::text').extract_first(),\n",
    "            'author': quote.css('small.author::text').extract_first(),\n",
    "            'tags': quote.css('div.tags a.tag::text').extract(),\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, run the command `scrapy crawl quotes -o quotes.json` to extract data from these pages and store the parsed results in the file `quotes.json`.\n",
    "\n",
    "Then take a look (e.g. `cat quotes.json`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are sort of scraping, but there is no *crawling* to speak of. What does that mean, crawling? Let's see how to follow links."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace the contents of `quotes_spider.py` with the code below. Note that this one also uses the `start_urls` shortcut (which you can read about on the tutorial page)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = \"quotes\"\n",
    "    start_urls = [\n",
    "        'http://quotes.toscrape.com/page/1/',\n",
    "    ]\n",
    "\n",
    "    def parse(self, response):\n",
    "        for quote in response.css('div.quote'):\n",
    "            yield {\n",
    "                'text': quote.css('span.text::text').extract_first(),\n",
    "                'author': quote.css('small.author::text').extract_first(),\n",
    "                'tags': quote.css('div.tags a.tag::text').extract(),\n",
    "            }\n",
    "\n",
    "        next_page = response.css('li.next a::attr(href)').extract_first()\n",
    "        if next_page is not None:\n",
    "            next_page = response.urljoin(next_page)\n",
    "            yield scrapy.Request(next_page, callback=self.parse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice what has changed: we have extracted links and included logic to move to the next one in the list. More precisely...\n",
    ">Now, after extracting the data, the `parse()` method looks for the link to the next page, builds a full absolute URL using the `urljoin()` method (since the links can be relative) and yields a new request to the next page, registering itself as callback to handle the data extraction for the next page and to keep the crawling going through all the pages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What else can you do? Here is one example of a slightly more advanced spider that scrapes author information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "\n",
    "class AuthorSpider(scrapy.Spider):\n",
    "    name = 'author'\n",
    "\n",
    "    start_urls = ['http://quotes.toscrape.com/']\n",
    "\n",
    "    def parse(self, response):\n",
    "        # follow links to author pages\n",
    "        for href in response.css('.author + a::attr(href)'):\n",
    "            yield response.follow(href, self.parse_author)\n",
    "\n",
    "        # follow pagination links\n",
    "        for href in response.css('li.next a::attr(href)'):\n",
    "            yield response.follow(href, self.parse)\n",
    "\n",
    "    def parse_author(self, response):\n",
    "        def extract_with_css(query):\n",
    "            return response.css(query).extract_first().strip()\n",
    "\n",
    "        yield {\n",
    "            'name': extract_with_css('h3.author-title::text'),\n",
    "            'birthdate': extract_with_css('.author-born-date::text'),\n",
    "            'bio': extract_with_css('.author-description::text'),\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the tutorial:\n",
    ">This spider will start from the main page, it will follow all the links to the authors pages calling the `parse_author` callback for each of them, and also the pagination links with the `parse` callback as we saw before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Twitter!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before anything, you will have to [sign up for developer access](https://developer.twitter.com/en/apply-for-access)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use *Python Twitter Tools* from *sixohsix* to get some data from Twitter. In order to use this notebook, you must install the python package `twitter`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to make this work, you will need to store your keys and tokens in a dicionary called \"auth_dict\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to need the `twitter` package. You can download the package [here](https://github.com/sixohsix/twitter) and use `easy_install`. But you can also find it in [*PyPi*](https://pypi.org/), i.e. you can install it using `pip`.\n",
    "\n",
    "To go the latter route, just make sure you conda environment is activated and then go like this:\n",
    "\n",
    "` pip install twitter `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from twitter import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('auth_dict','r') as f:\n",
    "    twtr_auth = json.load(f)\n",
    "    \n",
    "# To make it more readable, lets store\n",
    "# the OAuth credentials in strings first.\n",
    "CONSUMER_KEY = twtr_auth['consumer_key']\n",
    "CONSUMER_SECRET = twtr_auth['consumer_secret']\n",
    "OAUTH_TOKEN = twtr_auth['token']\n",
    "OAUTH_TOKEN_SECRET = twtr_auth['token_secret']\n",
    "    \n",
    "# Then, we store the OAuth object in \"auth\"\n",
    "auth = OAuth(OAUTH_TOKEN, OAUTH_TOKEN_SECRET,\n",
    "                           CONSUMER_KEY, CONSUMER_SECRET)\n",
    "# Notice that there are four tokens - you need to create these in the\n",
    "# Twitter Apps dashboard after you have created your own \"app\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create a `twitter` API wrapper object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now create the twitter search object.\n",
    "t = Twitter(auth=auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what is it, exactly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get some *statuses* from my timeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tmln = t.statuses.home_timeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Here](https://developer.twitter.com/en/docs/tweets/data-dictionary/overview/tweet-object.html) is the Twitter.com documentation on statuses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I get the timeline of another user, Zed Shaw. This might not work for you if you are not a follower of his."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeds_tmln = t.statuses.user_timeline(screen_name=\"zedshaw\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, here is another account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppoly_tmln = t.statuses.user_timeline(screen_name=\"primalpoly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppoly_tmln[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, now we'll break it down, starting from a more basic request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_example = \"\"\"GET https://api.twitter.com/1.1/search/tweets\n",
    "                .json?q=%23freebandnames&since_id=2401261998405\n",
    "                1000&max_id=250126199840518145&result_type=mixed&count=4\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we used triple quotes (i.e. 3 double quotes) for this multiline string. It is still just a string. What is going on in this string?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first piece of information in the string *docs_example* that stands out is \"freebandnames\". That's a search term, but what is its effect? It is preceded by \"%23\" to indicate *hashtags*. See [this](https://brajeshwar.github.io/entities/) for more.\n",
    "\n",
    "Rmk: [There is no such thing as plaintext.](https://www.joelonsoftware.com/2003/10/08/the-absolute-minimum-every-software-developer-absolutely-positively-must-know-about-unicode-and-character-sets-no-excuses/)\n",
    "\n",
    "Next, we see \"since_id\" - we are searching for tweets tweeted since a given tweet ID number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"max_id\" gives the other bound - we want no tweets with IDs greater than this value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are asking for \"mixed\" results - both popular and recent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count refers to results per page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**But we don't want to build such a string every time we do a query, and that is one reason for the API wrapper provided by `twitter`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "frebandnames = t.search.tweets(q='#freebandnames', since_id=24012619984051000,\n",
    "                result_type='mixed', count=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frebandnames.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "freeb = frebandnames['statuses']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freeb[0]['user']['screen_name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**More Examples**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get your \"home\" timeline\n",
    "t.statuses.home_timeline()\n",
    "\n",
    "# Get a particular friend's timeline\n",
    "t.statuses.user_timeline(screen_name=\"zedshaw\")\n",
    "\n",
    "# to pass in GET/POST parameters, such as `count`\n",
    "t.statuses.home_timeline(count=5)\n",
    "\n",
    "# to pass in the GET/POST parameter `id` you need to use `_id`\n",
    "t.statuses.oembed(_id=1234567890)\n",
    "\n",
    "# Update your status\n",
    "t.statuses.update(\n",
    "    status=\"Here is another tweet.\")\n",
    "\n",
    "# Send a direct message\n",
    "#t.direct_messages.new(\n",
    "#    user=\"primalpoly\",\n",
    "#    text=\"Geoffy-baby, big fan of your work.\") # Try not to spam him, guys\n",
    "\n",
    "# Get the members of tamtar's list \"Things That Are Rad\"\n",
    "t.lists.members(owner_screen_name=\"buffer\", slug=\"the-buffer-team\")\n",
    "\n",
    "# An *optional* `_timeout` parameter can also be used for API\n",
    "# calls which take much more time than normal or twitter stops\n",
    "# responding for some reason:\n",
    "#t.users.lookup(\n",
    "#    screen_name=','.join(A_LIST_OF_100_SCREEN_NAMES), _timeout=1)\n",
    "\n",
    "# Rmk: A_LIST_OF_100_SCREEN_NAMES is not defined. Why not fix that?\n",
    "\n",
    "# Overriding Method: GET/POST\n",
    "# you should not need to use this method as this library properly\n",
    "# detects whether GET or POST should be used, Nevertheless\n",
    "# to force a particular method, use `_method`\n",
    "t.statuses.oembed(_id=1234567890, _method='GET')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for the latest tweets about #pycon\n",
    "t.search.tweets(q=\"#pycon\")\n",
    "\n",
    "# Search for the latest tweets about #pycon, using extended mode\n",
    "t.search.tweets(q=\"#pycon\", tweet_mode='extended')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can search trends by geographic region. The Yahoo! Where On Earth ID for the entire world is 1, for example. Look [here](https://developer.twitter.com/en/docs/trends/trends-for-location/api-reference/get-trends-place) for more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Streaming**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rmk: this cell will not run!\n",
    "twitter_stream = TwitterStream(auth=OAuth(...))\n",
    "iterator = twitter_stream.statuses.sample()\n",
    "\n",
    "for tweet in iterator:\n",
    "    #...do something with this tweet..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a *streaming* connection (not RESTful, different from Search).\n",
    "t_stream = TwitterStream(auth=auth)\n",
    "\n",
    "\n",
    "# Get an *iterator* object from the twitter wrapper\n",
    "\n",
    "tweeterator = t_stream.statuses.sample()\n",
    "\n",
    "\n",
    "# The loop below simply prints randomly selected new tweets\n",
    "# until we reach the threshold of \"tweet_count\"\n",
    "\n",
    "tweet_count = 100\n",
    "for tweet in tweeterator:\n",
    "    tweet_count -= 1\n",
    "    print(json.dumps(tweet))  \n",
    "    if tweet_count <= 0:\n",
    "        break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORLD_WOE_ID = 1\n",
    "US_WOE_ID = 23424977"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get the trends (top 50) for the whole world, and then for the US."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prefix ID with the underscore for query string parameterization.\n",
    "# Without the underscore, the twitter package appends the ID value\n",
    "# to the URL itself as a special case keyword argument.\n",
    "\n",
    "world_trends = t.trends.place(_id=WORLD_WOE_ID)\n",
    "us_trends = t.trends.place(_id=US_WOE_ID)\n",
    "\n",
    "print(world_trends)\n",
    "print('\\n')\n",
    "print(us_trends)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That doesn't look so good, so now we \"pretty print\" it using the JSON module. The builtin *print* gives better output when its input is a JSON object (rather than, say, a string or list object)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(us_trends)[0]['trends'][4]['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(json.dumps(world_trends, indent=1))\n",
    "print('\\n')\n",
    "print(json.dumps(us_trends, indent=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have both results in memory, we can combine them. For example, we find the intersection of the top world trends and the top USA trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why bother with \"set\" here?\n",
    "world_trends_set = set([trend['name'] for trend in world_trends[0]['trends']])\n",
    "\n",
    "us_trends_set = set([trend['name'] for trend in us_trends[0]['trends']]) \n",
    "\n",
    "common_trends = world_trends_set.intersection(us_trends_set) # intersection is a set method\n",
    "\n",
    "print(common_trends)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's collect search results in a loop. Note that we are already able to do this with the Search API and that the Streaming API will be needed for larger volume."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *count* parameter represents the number of tweets we want *per page*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is important because the Twitter REST API uses *cursoring* to organize large search results in *pages*. The next example shows how to use the cursor. Read more about it [here](https://developer.twitter.com/en/docs/basics/cursoring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the next variable to any trending topic\n",
    "# or anything else for that matter.\n",
    "q = '\"Super Bowl\"' \n",
    "\n",
    "# Same as before - the number of tweets we want *per page*.\n",
    "count = 1000\n",
    "\n",
    "# We do the search call.\n",
    "search_results = t.search.tweets(q=q, count=count)\n",
    "\n",
    "# Remember 'status' refers to the actual content of a tweet.\n",
    "# (as opposed to the metadata)\n",
    "statuses = search_results['statuses']\n",
    "\n",
    "\n",
    "# Iterate through 5 more batches of results by following the cursor.\n",
    "# The use of the underscore \"_\" below is a convention in python \n",
    "# indicating to the reader that the variable will not be used for\n",
    "# anything within the loop.\n",
    "for _ in range(5):\n",
    "    print(\"Length of statuses \", len(statuses))\n",
    "    # Remember \"try...except\"? Here it is in action:\n",
    "    try:\n",
    "        next_results = search_results['search_metadata']['next_results']\n",
    "    # No more results when next_results doesn't exist\n",
    "    except KeyError as e: \n",
    "        break\n",
    "        \n",
    "    # Create a dictionary from next_results, which has the following form:\n",
    "    # ?max_id=313519052523986943&q=NCAA&include_entities=1\n",
    "    kwargs = dict([ kv.split('=') for kv in next_results[1:].split(\"&\") ]) \n",
    "    \n",
    "    search_results = t.search.tweets(**kwargs) #Another API call\n",
    "    statuses += search_results['statuses'] #Appending to statuses results\n",
    "\n",
    "# Show one sample search result by slicing the list...\n",
    "print(json.dumps(statuses[0], indent=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the \"except\" line in the cell above. Including \"as e\" after the \"KeyError\" lets you access the attributes of the error - we didn't do that, but you might find a need for it. Inside the indent, you could manipulate the object e and get its attributes, such as e.args.\n",
    "\n",
    "Instead, we simply break out of the loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next example, we extract text, screen names and hashtags from tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(statuses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status_texts = [ status['text'] for status in statuses ]\n",
    "\n",
    "screen_names = [ user_mention['screen_name'] for status in statuses\n",
    "                     for user_mention in status['entities']['user_mentions'] ]\n",
    "\n",
    "hashtags = [ hashtag['text'] for status in statuses\n",
    "                 for hashtag in status['entities']['hashtags'] ]\n",
    "\n",
    "# Compute a collection of all words from all tweets\n",
    "words = [ w for t in status_texts for w in t.split() ]\n",
    "\n",
    "# Explore the first 5 items for each...\n",
    "\n",
    "print(json.dumps(status_texts[0:5], indent=1))\n",
    "print(json.dumps(screen_names[0:5], indent=1))\n",
    "print(json.dumps(hashtags[0:5], indent=1))\n",
    "print(json.dumps(words[0:5], indent=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create a basic frequency distribution from words in tweets - first glimpse at NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Recall that you can loop through any\n",
    "# iterator in python - such as a list\n",
    "# of lists!\n",
    "for item in [words, screen_names, hashtags]:\n",
    "    c = Counter(item)\n",
    "    print(c.most_common()[:10]) # top 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function for computing lexical diversity\n",
    "def lexical_diversity(tokens):\n",
    "    return 1.0*len(set(tokens))/len(tokens) \n",
    "\n",
    "# A function for computing the average number of words per tweet\n",
    "def average_words(statuses):\n",
    "    total_words = sum([ len(s.split()) for s in statuses ]) \n",
    "    return total_words/len(statuses)\n",
    "\n",
    "print(lexical_diversity(words))\n",
    "print(lexical_diversity(screen_names))\n",
    "print(lexical_diversity(hashtags))\n",
    "print(average_words(status_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# First, we get the frequencies in order.\n",
    "word_counts = sorted(Counter(words).values(), reverse=True)\n",
    "\n",
    "# We can plot it along log-scaled axes\n",
    "plt.loglog(word_counts)\n",
    "plt.ylabel(\"Freq\")\n",
    "plt.xlabel(\"Word Rank\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here that is in `bokeh`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.plotting import figure, show, output_notebook\n",
    "output_notebook()\n",
    "\n",
    "p = figure(title=\"log axis example\", y_axis_type=\"log\", x_axis_type='log',\n",
    "           y_range=(.9, 400))\n",
    "\n",
    "p.line(range(len(word_counts)), word_counts, legend=\"word counts\",\n",
    "       line_color=\"blue\")\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also do histograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label, data in (('Words', words), \n",
    "                    ('Screen Names', screen_names), \n",
    "                    ('Hashtags', hashtags)):\n",
    "\n",
    "    # Build a frequency map for each set of data\n",
    "    # and plot the values\n",
    "    c = Counter(data)\n",
    "    plt.hist(c.values())\n",
    "    \n",
    "    # Add a title and y-label ...\n",
    "    plt.title(label)\n",
    "    plt.ylabel(\"Number of items in bin\")\n",
    "    plt.xlabel(\"Bins (number of times an item appeared)\")\n",
    "    \n",
    "    # ... and display as a new figure\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** Do the above in `bokeh`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now for something completely different...finding the most popular retweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retweets = [\n",
    "            # We are building a list of tuples\n",
    "            (status['retweet_count'], \n",
    "             status['retweeted_status']['user']['screen_name'],\n",
    "             status['text'],\n",
    "             status['retweeted_status']) \n",
    "            \n",
    "            # ... for each status ...\n",
    "            for status in statuses \n",
    "            \n",
    "            # ... so long as the status meets this condition.\n",
    "                if 'retweeted_status' in status\n",
    "           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking up users who have retweeted a 'status'.\n",
    "\n",
    "First, let's pick one of the retweets. We will find the id of the original tweet from the 'retweeted_status' node (the last element of our tuple)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retweets[0][-1]['id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the id of the retweeted tweet. We will use this to search Twitter again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that we are doing another search here.\n",
    "rtwtd_id = 1047848885073403904 # Fill this in\n",
    "_retweets = t.statuses.retweets(id=rtwtd_id)\n",
    "print([r['user']['screen_name'] for r in _retweets])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = [count for count, _, _, _ in retweets]\n",
    "\n",
    "plt.hist(counts)\n",
    "plt.title(\"Retweets\")\n",
    "plt.xlabel('Bins (number of times retweeted)')\n",
    "plt.ylabel('Number of tweets in bin')\n",
    "\n",
    "print(counts)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Twitter Functions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to reuse the code from the cells above. Also, we want to write clean and readable code when we implement longer and more complex procedures. Here is a function we have already used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def oauth_login():\n",
    "    # Replace credentials below with appropriate values - \n",
    "    # this works in the TwitterAPIcontd notebook because\n",
    "    # we defined the credentials above\n",
    "    CONSUMER_KEY = CONSUMER_KEY\n",
    "    CONSUMER_SECRET = CONSUMER_SECRET\n",
    "    OAUTH_TOKEN = OAUTH_TOKEN\n",
    "    OAUTH_TOKEN_SECRET = OAUTH_TOKEN_SECRET\n",
    "    auth = twitter.oauth.OAuth(OAUTH_TOKEN, OAUTH_TOKEN_SECRET,\n",
    "    CONSUMER_KEY, CONSUMER_SECRET)\n",
    "    twitter_api = twitter.Twitter(auth=auth)\n",
    "    return twitter_api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can wrap the lines that give us trends in a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def twitter_trends(twitter_api, woe_id):\n",
    "    # Prefix ID with the underscore for query string parameterization.\n",
    "    # Without the underscore, the twitter package appends the ID value\n",
    "    # to the URL itself as a special-case keyword argument.\n",
    "    return twitter_api.trends.place(_id=woe_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Likewise, we define a function for the looped twitter search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def twitter_search(twitter_api, q, max_results=200, **kw):\n",
    "    search_results = twitter_api.search.tweets(q=q, count=100, **kw)\n",
    "    statuses = search_results['statuses']\n",
    "    # Enforce a reasonable limit\n",
    "    max_results = min(1000, max_results)\n",
    "    for _ in range(10): # 10*100 = 1000\n",
    "        try:\n",
    "            next_results = search_results['search_metadata']['next_results']\n",
    "        except KeyError as e: # No more results when next_results doesn't exist\n",
    "            break\n",
    "        # Create a dictionary from next_results, which has the following form:\n",
    "        # ?max_id=313519052523986943&q=NCAA&include_entities=1\n",
    "        kwargs = dict([ kv.split('=') \n",
    "                       for kv in next_results[1:].split(\"&\") ])\n",
    "        search_results = twitter_api.search.tweets(**kwargs)\n",
    "        statuses += search_results['statuses']\n",
    "        if len(statuses) > max_results:\n",
    "            break\n",
    "    \n",
    "    return statuses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will likely encounter errors in mining Twitter data. Here is a function to automate the handling of certain errors. See *Mining the Social Web* for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "from twitter.api import TwitterHTTPError\n",
    "from urllib.error import URLError\n",
    "from http.client import BadStatusLine\n",
    "\n",
    "def make_twitter_request(twitter_api_func, max_errors=10, *args, **kw):\n",
    "    # A nested helper function that handles common HTTPErrors. Return an updated\n",
    "    # value for wait_period if the problem is a 500 level error. Block until the\n",
    "    # rate limit is reset if it's a rate limiting issue (429 error). Returns None\n",
    "    # for 401 and 404 errors, which requires special handling by the caller.\n",
    "    def handle_twitter_http_error(e, wait_period=2, sleep_when_rate_limited=True):\n",
    "        if wait_period > 3600: # Seconds\n",
    "            print('Too many retries. Quitting.', file=sys.stderr)\n",
    "            raise e\n",
    "        if e.e.code == 401:\n",
    "            return None\n",
    "        elif e.e.code == 404:\n",
    "            print('Encountered 404 Error (Not Found)', file=sys.stderr)\n",
    "            return None\n",
    "        elif e.e.code == 429:\n",
    "            print('Encountered 429 Error (Rate Limit Exceeded)', file=sys.stderr)\n",
    "            if sleep_when_rate_limited:\n",
    "                print(\"Retrying in 15 minutes...ZzZ...\", file=sys.stderr)\n",
    "                sys.stderr.flush()\n",
    "                time.sleep(60*15 + 5)\n",
    "                print('...ZzZ...Awake now and trying again.', file=sys.stderr)\n",
    "                return 2\n",
    "            else:\n",
    "                raise e # Caller must handle the rate limiting issue\n",
    "        elif e.e.code in (500, 502, 503, 504):\n",
    "            print('Encountered %i Error. Retrying in %i seconds' % (e.e.code, wait_period), file=sys.stderr)\n",
    "            time.sleep(wait_period)\n",
    "            wait_period *= 1.5\n",
    "            return wait_period\n",
    "        else:\n",
    "            raise e\n",
    "\n",
    "    # End of nested helper function\n",
    "\n",
    "    wait_period = 2\n",
    "    error_count = 0\n",
    "    while True:\n",
    "        try:\n",
    "            return twitter_api_func(*args, **kw)\n",
    "        except TwitterHTTPError as e:\n",
    "            error_count = 0\n",
    "            wait_period = handle_twitter_http_error(e, wait_period)\n",
    "            if wait_period is None:\n",
    "                return\n",
    "        except URLError as e:\n",
    "            error_count += 1\n",
    "            print(\"URLError encountered. Continuing.\", file=sys.stderr)\n",
    "            if error_count > max_errors:\n",
    "                print(\"Too many consecutive errors...bailing out.\", file=sys.stderr)\n",
    "                raise\n",
    "        except BadStatusLine as e:\n",
    "            error_count += 1\n",
    "            print >> sys.stderr, \"BadStatusLine encountered. Continuing.\"\n",
    "            if error_count > max_errors:\n",
    "                print(\"Too many consecutive errors...bailing out.\", file=sys.stderr)\n",
    "                raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = make_twitter_request(t.users.lookup, screen_name=\"SocialWebMining\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will want to write responses to disk, on the fly, so that we can collect many observations for later analysis. In *Mining the Social Web*, the Mongo DB database program is recommended. Here is another way (that also can be adapted so that it writes to a database)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's wrap the extraction of tweet \"[entities](https://developer.twitter.com/en/docs/tweets/data-dictionary/overview/entities-object)\" in a function which takes a list of statuses as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tweet_entities(statuses):\n",
    "    if len(statuses) == 0:\n",
    "        return [], [], [], [], []\n",
    "    screen_names = [ user_mention['screen_name'] \n",
    "                    for status in statuses\n",
    "                        for user_mention in status['entities']['user_mentions'] ]\n",
    "    hashtags = [ hashtag['text']\n",
    "                    for status in statuses\n",
    "                        for hashtag in status['entities']['hashtags'] ]\n",
    "    urls = [ url['expanded_url']\n",
    "                    for status in statuses\n",
    "                        for url in status['entities']['urls'] ]\n",
    "    symbols = [ symbol['text']\n",
    "                    for status in statuses\n",
    "                        for symbol in status['entities']['symbols'] ]\n",
    "    if status['entities'].has_key('media'):\n",
    "        media = [ media['url']\n",
    "            for status in statuses\n",
    "                for media in status['entities']['media'] ]\n",
    "    else:\n",
    "        media = []\n",
    "    return screen_names, hashtags, urls, media, symbols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above function is a good template of sorts. You can use it to develop your own function for extracting other information from a *list of statuses*. \n",
    "\n",
    "Let's define two more functions, then we'll sample from the Twitter stream, writing results as we go. The functions below will take statuses, extract some information, and then write the results into a CSV file using Pandas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tweet_basics(status):\n",
    "    screen_name = None\n",
    "    tweet_ID = None\n",
    "    text = None\n",
    "    if 'user' in status:\n",
    "        screen_name = status['user']['screen_name'] \n",
    "        tweet_ID = status['id']\n",
    "        text = status['text']\n",
    "    return screen_name, tweet_ID, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_to_csv(file, status):\n",
    "    screen_name, tweet_ID, text = extract_tweet_basics(status)\n",
    "    df = pd.DataFrame([[screen_name,tweet_ID,text]], columns=['screen_name','tweet_ID','text'])\n",
    "    with open(file, 'a') as f:\n",
    "        df.to_csv(f,header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we stick this in the streaming loop from last time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a *streaming* connection (not RESTful, different from Search).\n",
    "t_stream = TwitterStream(auth=auth)\n",
    "\n",
    "\n",
    "# Get an *iterator* object from the twitter wrapper\n",
    "\n",
    "tweeterator = t_stream.statuses.sample()\n",
    "\n",
    "# Create a CSV file with column names\n",
    "# but no data (yet).\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(columns=['screen_name','tweet_ID','text'])\n",
    "df.to_csv('my_csv.csv', index=False)\n",
    "\n",
    "\n",
    "# The loop below will grab a new tweet,\n",
    "# extract some basic info, put that info\n",
    "# in a dataframe object, then use that\n",
    "# dataframe object to append one row to\n",
    "# the existing CSV file, 'my_csv.csv'.\n",
    "\n",
    "tweet_count = 100\n",
    "for tweet in tweeterator:\n",
    "    tweet_count -= 1\n",
    "    tweet_to_csv('my_csv.csv', tweet)  \n",
    "    if tweet_count <= 0:\n",
    "        break "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
