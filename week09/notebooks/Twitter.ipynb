{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIS600 - Social Media Data Mining \n",
    "###  \n",
    "<img src=\"https://www.syracuse.edu/wp-content/themes/g6-carbon/img/syracuse-university-seal.svg?ver=6.3.9\" style=\"width: 200px;\"/>\n",
    "\n",
    "# Twitter\n",
    "\n",
    "###  October 4, 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today we'll use *Python Twitter Tools* from *sixohsix* to get some data from Twitter. In order to use this notebook, you must install the python package `twitter`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to make this work, you will need to store your keys and tokens in a dicionary called \"auth_dict\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to need the `twitter` package. You can download the package [here](https://github.com/sixohsix/twitter) and use `easy_install`. But you can also find it in [*PyPi*](https://pypi.org/), i.e. you can install it using `pip`.\n",
    "\n",
    "To go the latter route, just make sure you conda environment is activated and then go like this:\n",
    "\n",
    "` pip install twitter `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from twitter import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('auth_dict','r') as f:\n",
    "    twtr_auth = json.load(f)\n",
    "    \n",
    "# To make it more readable, lets store\n",
    "# the OAuth credentials in strings first.\n",
    "CONSUMER_KEY = twtr_auth['consumer_key']\n",
    "CONSUMER_SECRET = twtr_auth['consumer_secret']\n",
    "OAUTH_TOKEN = twtr_auth['token']\n",
    "OAUTH_TOKEN_SECRET = twtr_auth['token_secret']\n",
    "    \n",
    "# Then, we store the OAuth object in \"auth\"\n",
    "auth = OAuth(OAUTH_TOKEN, OAUTH_TOKEN_SECRET,\n",
    "                           CONSUMER_KEY, CONSUMER_SECRET)\n",
    "# Notice that there are four tokens - you need to create these in the\n",
    "# Twitter Apps dashboard after you have created your own \"app\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create a `twitter` API wrapper object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now create the twitter search object.\n",
    "t = Twitter(auth=auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what is it, exactly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get some *statuses* from my timeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tmln = t.statuses.home_timeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Here](https://developer.twitter.com/en/docs/tweets/data-dictionary/overview/tweet-object.html) is the Twitter.com documentation on statuses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I get the timeline of another user, Zed Shaw. This might not work for you if you are not a follower of his."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeds_tmln = t.statuses.user_timeline(screen_name=\"zedshaw\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, here is another account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppoly_tmln = t.statuses.user_timeline(screen_name=\"primalpoly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppoly_tmln[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, now we'll break it down, starting from a more basic request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_example = \"\"\"GET https://api.twitter.com/1.1/search/tweets\n",
    "                .json?q=%23freebandnames&since_id=2401261998405\n",
    "                1000&max_id=250126199840518145&result_type=mixed&count=4\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we used triple quotes (i.e. 3 double quotes) for this multiline string. It is still just a string. What is going on in this string?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first piece of information in the string *docs_example* that stands out is \"freebandnames\". That's a search term, but what is its effect? It is preceded by \"%23\" to indicate *hashtags*. See [this](https://brajeshwar.github.io/entities/) for more.\n",
    "\n",
    "Rmk: [There is no such thing as plaintext.](https://www.joelonsoftware.com/2003/10/08/the-absolute-minimum-every-software-developer-absolutely-positively-must-know-about-unicode-and-character-sets-no-excuses/)\n",
    "\n",
    "Next, we see \"since_id\" - we are searching for tweets tweeted since a given tweet ID number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"max_id\" gives the other bound - we want no tweets with IDs greater than this value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are asking for \"mixed\" results - both popular and recent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count refers to results per page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**But we don't want to build such a string every time we do a query, and that is one reason for the API wrapper provided by `twitter`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "frebandnames = t.search.tweets(q='#freebandnames', since_id=24012619984051000,\n",
    "                result_type='mixed', count=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frebandnames.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "freeb = frebandnames['statuses']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freeb[0]['user']['screen_name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**More Examples**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get your \"home\" timeline\n",
    "t.statuses.home_timeline()\n",
    "\n",
    "# Get a particular friend's timeline\n",
    "t.statuses.user_timeline(screen_name=\"zedshaw\")\n",
    "\n",
    "# to pass in GET/POST parameters, such as `count`\n",
    "t.statuses.home_timeline(count=5)\n",
    "\n",
    "# to pass in the GET/POST parameter `id` you need to use `_id`\n",
    "t.statuses.oembed(_id=1234567890)\n",
    "\n",
    "# Update your status\n",
    "t.statuses.update(\n",
    "    status=\"Here is another tweet.\")\n",
    "\n",
    "# Send a direct message\n",
    "#t.direct_messages.new(\n",
    "#    user=\"primalpoly\",\n",
    "#    text=\"Geoffy-baby, big fan of your work.\") # Try not to spam him, guys\n",
    "\n",
    "# Get the members of tamtar's list \"Things That Are Rad\"\n",
    "t.lists.members(owner_screen_name=\"buffer\", slug=\"the-buffer-team\")\n",
    "\n",
    "# An *optional* `_timeout` parameter can also be used for API\n",
    "# calls which take much more time than normal or twitter stops\n",
    "# responding for some reason:\n",
    "#t.users.lookup(\n",
    "#    screen_name=','.join(A_LIST_OF_100_SCREEN_NAMES), _timeout=1)\n",
    "\n",
    "# Rmk: A_LIST_OF_100_SCREEN_NAMES is not defined. Why not fix that?\n",
    "\n",
    "# Overriding Method: GET/POST\n",
    "# you should not need to use this method as this library properly\n",
    "# detects whether GET or POST should be used, Nevertheless\n",
    "# to force a particular method, use `_method`\n",
    "t.statuses.oembed(_id=1234567890, _method='GET')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for the latest tweets about #pycon\n",
    "t.search.tweets(q=\"#pycon\")\n",
    "\n",
    "# Search for the latest tweets about #pycon, using extended mode\n",
    "t.search.tweets(q=\"#pycon\", tweet_mode='extended')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can search trends by geographic region. The Yahoo! Where On Earth ID for the entire world is 1, for example. Look [here](https://developer.twitter.com/en/docs/trends/trends-for-location/api-reference/get-trends-place) for more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Streaming**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rmk: this cell will not run!\n",
    "twitter_stream = TwitterStream(auth=OAuth(...))\n",
    "iterator = twitter_stream.statuses.sample()\n",
    "\n",
    "for tweet in iterator:\n",
    "    #...do something with this tweet..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a *streaming* connection (not RESTful, different from Search).\n",
    "t_stream = TwitterStream(auth=auth)\n",
    "\n",
    "\n",
    "# Get an *iterator* object from the twitter wrapper\n",
    "\n",
    "tweeterator = t_stream.statuses.sample()\n",
    "\n",
    "\n",
    "# The loop below simply prints randomly selected new tweets\n",
    "# until we reach the threshold of \"tweet_count\"\n",
    "\n",
    "tweet_count = 100\n",
    "for tweet in tweeterator:\n",
    "    tweet_count -= 1\n",
    "    print(json.dumps(tweet))  \n",
    "    if tweet_count <= 0:\n",
    "        break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORLD_WOE_ID = 1\n",
    "US_WOE_ID = 23424977"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get the trends (top 50) for the whole world, and then for the US."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prefix ID with the underscore for query string parameterization.\n",
    "# Without the underscore, the twitter package appends the ID value\n",
    "# to the URL itself as a special case keyword argument.\n",
    "\n",
    "world_trends = t.trends.place(_id=WORLD_WOE_ID)\n",
    "us_trends = t.trends.place(_id=US_WOE_ID)\n",
    "\n",
    "print(world_trends)\n",
    "print('\\n')\n",
    "print(us_trends)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That doesn't look so good, so now we \"pretty print\" it using the JSON module. The builtin *print* gives better output when its input is a JSON object (rather than, say, a string or list object)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(us_trends)[0]['trends'][4]['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(json.dumps(world_trends, indent=1))\n",
    "print('\\n')\n",
    "print(json.dumps(us_trends, indent=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have both results in memory, we can combine them. For example, we find the intersection of the top world trends and the top USA trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why bother with \"set\" here?\n",
    "world_trends_set = set([trend['name'] for trend in world_trends[0]['trends']])\n",
    "\n",
    "us_trends_set = set([trend['name'] for trend in us_trends[0]['trends']]) \n",
    "\n",
    "common_trends = world_trends_set.intersection(us_trends_set) # intersection is a set method\n",
    "\n",
    "print(common_trends)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's collect search results in a loop. Note that we are already able to do this with the Search API and that the Streaming API will be needed for larger volume."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *count* parameter represents the number of tweets we want *per page*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is important because the Twitter REST API uses *cursoring* to organize large search results in *pages*. The next example shows how to use the cursor. Read more about it [here](https://developer.twitter.com/en/docs/basics/cursoring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the next variable to any trending topic\n",
    "# or anything else for that matter.\n",
    "q = '\"Super Bowl\"' \n",
    "\n",
    "# Same as before - the number of tweets we want *per page*.\n",
    "count = 1000\n",
    "\n",
    "# We do the search call.\n",
    "search_results = t.search.tweets(q=q, count=count)\n",
    "\n",
    "# Remember 'status' refers to the actual content of a tweet.\n",
    "# (as opposed to the metadata)\n",
    "statuses = search_results['statuses']\n",
    "\n",
    "\n",
    "# Iterate through 5 more batches of results by following the cursor.\n",
    "# The use of the underscore \"_\" below is a convention in python \n",
    "# indicating to the reader that the variable will not be used for\n",
    "# anything within the loop.\n",
    "for _ in range(5):\n",
    "    print(\"Length of statuses \", len(statuses))\n",
    "    # Remember \"try...except\"? Here it is in action:\n",
    "    try:\n",
    "        next_results = search_results['search_metadata']['next_results']\n",
    "    # No more results when next_results doesn't exist\n",
    "    except KeyError as e: \n",
    "        break\n",
    "        \n",
    "    # Create a dictionary from next_results, which has the following form:\n",
    "    # ?max_id=313519052523986943&q=NCAA&include_entities=1\n",
    "    kwargs = dict([ kv.split('=') for kv in next_results[1:].split(\"&\") ]) \n",
    "    \n",
    "    search_results = t.search.tweets(**kwargs) #Another API call\n",
    "    statuses += search_results['statuses'] #Appending to statuses results\n",
    "\n",
    "# Show one sample search result by slicing the list...\n",
    "print(json.dumps(statuses[0], indent=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the \"except\" line in the cell above. Including \"as e\" after the \"KeyError\" lets you access the attributes of the error - we didn't do that, but you might find a need for it. Inside the indent, you could manipulate the object e and get its attributes, such as e.args.\n",
    "\n",
    "Instead, we simply break out of the loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next example, we extract text, screen names and hashtags from tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(statuses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status_texts = [ status['text'] for status in statuses ]\n",
    "\n",
    "screen_names = [ user_mention['screen_name'] for status in statuses\n",
    "                     for user_mention in status['entities']['user_mentions'] ]\n",
    "\n",
    "hashtags = [ hashtag['text'] for status in statuses\n",
    "                 for hashtag in status['entities']['hashtags'] ]\n",
    "\n",
    "# Compute a collection of all words from all tweets\n",
    "words = [ w for t in status_texts for w in t.split() ]\n",
    "\n",
    "# Explore the first 5 items for each...\n",
    "\n",
    "print(json.dumps(status_texts[0:5], indent=1))\n",
    "print(json.dumps(screen_names[0:5], indent=1))\n",
    "print(json.dumps(hashtags[0:5], indent=1))\n",
    "print(json.dumps(words[0:5], indent=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create a basic frequency distribution from words in tweets - first glimpse at NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Recall that you can loop through any\n",
    "# iterator in python - such as a list\n",
    "# of lists!\n",
    "for item in [words, screen_names, hashtags]:\n",
    "    c = Counter(item)\n",
    "    print(c.most_common()[:10]) # top 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function for computing lexical diversity\n",
    "def lexical_diversity(tokens):\n",
    "    return 1.0*len(set(tokens))/len(tokens) \n",
    "\n",
    "# A function for computing the average number of words per tweet\n",
    "def average_words(statuses):\n",
    "    total_words = sum([ len(s.split()) for s in statuses ]) \n",
    "    return total_words/len(statuses)\n",
    "\n",
    "print(lexical_diversity(words))\n",
    "print(lexical_diversity(screen_names))\n",
    "print(lexical_diversity(hashtags))\n",
    "print(average_words(status_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# First, we get the frequencies in order.\n",
    "word_counts = sorted(Counter(words).values(), reverse=True)\n",
    "\n",
    "# We can plot it along log-scaled axes\n",
    "plt.loglog(word_counts)\n",
    "plt.ylabel(\"Freq\")\n",
    "plt.xlabel(\"Word Rank\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here that is in `bokeh`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.plotting import figure, show, output_notebook\n",
    "output_notebook()\n",
    "\n",
    "p = figure(title=\"log axis example\", y_axis_type=\"log\", x_axis_type='log',\n",
    "           y_range=(.9, 400))\n",
    "\n",
    "p.line(range(len(word_counts)), word_counts, legend=\"word counts\",\n",
    "       line_color=\"blue\")\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also do histograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label, data in (('Words', words), \n",
    "                    ('Screen Names', screen_names), \n",
    "                    ('Hashtags', hashtags)):\n",
    "\n",
    "    # Build a frequency map for each set of data\n",
    "    # and plot the values\n",
    "    c = Counter(data)\n",
    "    plt.hist(c.values())\n",
    "    \n",
    "    # Add a title and y-label ...\n",
    "    plt.title(label)\n",
    "    plt.ylabel(\"Number of items in bin\")\n",
    "    plt.xlabel(\"Bins (number of times an item appeared)\")\n",
    "    \n",
    "    # ... and display as a new figure\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** Do the above in `bokeh`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now for something completely different...finding the most popular retweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retweets = [\n",
    "            # We are building a list of tuples\n",
    "            (status['retweet_count'], \n",
    "             status['retweeted_status']['user']['screen_name'],\n",
    "             status['text'],\n",
    "             status['retweeted_status']) \n",
    "            \n",
    "            # ... for each status ...\n",
    "            for status in statuses \n",
    "            \n",
    "            # ... so long as the status meets this condition.\n",
    "                if 'retweeted_status' in status\n",
    "           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking up users who have retweeted a 'status'.\n",
    "\n",
    "First, let's pick one of the retweets. We will find the id of the original tweet from the 'retweeted_status' node (the last element of our tuple)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retweets[0][-1]['id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the id of the retweeted tweet. We will use this to search Twitter again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that we are doing another search here.\n",
    "rtwtd_id = 1047848885073403904 # Fill this in\n",
    "_retweets = t.statuses.retweets(id=rtwtd_id)\n",
    "print([r['user']['screen_name'] for r in _retweets])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = [count for count, _, _, _ in retweets]\n",
    "\n",
    "plt.hist(counts)\n",
    "plt.title(\"Retweets\")\n",
    "plt.xlabel('Bins (number of times retweeted)')\n",
    "plt.ylabel('Number of tweets in bin')\n",
    "\n",
    "print(counts)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Twitter Functions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to reuse the code from the cells above. Also, we want to write clean and readable code when we implement longer and more complex procedures. Here is a function we have already used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def oauth_login():\n",
    "    # Replace credentials below with appropriate values - \n",
    "    # this works in the TwitterAPIcontd notebook because\n",
    "    # we defined the credentials above\n",
    "    CONSUMER_KEY = CONSUMER_KEY\n",
    "    CONSUMER_SECRET = CONSUMER_SECRET\n",
    "    OAUTH_TOKEN = OAUTH_TOKEN\n",
    "    OAUTH_TOKEN_SECRET = OAUTH_TOKEN_SECRET\n",
    "    auth = twitter.oauth.OAuth(OAUTH_TOKEN, OAUTH_TOKEN_SECRET,\n",
    "    CONSUMER_KEY, CONSUMER_SECRET)\n",
    "    twitter_api = twitter.Twitter(auth=auth)\n",
    "    return twitter_api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can wrap the lines that give us trends in a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def twitter_trends(twitter_api, woe_id):\n",
    "    # Prefix ID with the underscore for query string parameterization.\n",
    "    # Without the underscore, the twitter package appends the ID value\n",
    "    # to the URL itself as a special-case keyword argument.\n",
    "    return twitter_api.trends.place(_id=woe_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Likewise, we define a function for the looped twitter search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def twitter_search(twitter_api, q, max_results=200, **kw):\n",
    "    search_results = twitter_api.search.tweets(q=q, count=100, **kw)\n",
    "    statuses = search_results['statuses']\n",
    "    # Enforce a reasonable limit\n",
    "    max_results = min(1000, max_results)\n",
    "    for _ in range(10): # 10*100 = 1000\n",
    "        try:\n",
    "            next_results = search_results['search_metadata']['next_results']\n",
    "        except KeyError as e: # No more results when next_results doesn't exist\n",
    "            break\n",
    "        # Create a dictionary from next_results, which has the following form:\n",
    "        # ?max_id=313519052523986943&q=NCAA&include_entities=1\n",
    "        kwargs = dict([ kv.split('=') \n",
    "                       for kv in next_results[1:].split(\"&\") ])\n",
    "        search_results = twitter_api.search.tweets(**kwargs)\n",
    "        statuses += search_results['statuses']\n",
    "        if len(statuses) > max_results:\n",
    "            break\n",
    "    \n",
    "    return statuses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will likely encounter errors in mining Twitter data. Here is a function to automate the handling of certain errors. See *Mining the Social Web* for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "from twitter.api import TwitterHTTPError\n",
    "from urllib.error import URLError\n",
    "from http.client import BadStatusLine\n",
    "\n",
    "def make_twitter_request(twitter_api_func, max_errors=10, *args, **kw):\n",
    "    # A nested helper function that handles common HTTPErrors. Return an updated\n",
    "    # value for wait_period if the problem is a 500 level error. Block until the\n",
    "    # rate limit is reset if it's a rate limiting issue (429 error). Returns None\n",
    "    # for 401 and 404 errors, which requires special handling by the caller.\n",
    "    def handle_twitter_http_error(e, wait_period=2, sleep_when_rate_limited=True):\n",
    "        if wait_period > 3600: # Seconds\n",
    "            print('Too many retries. Quitting.', file=sys.stderr)\n",
    "            raise e\n",
    "        if e.e.code == 401:\n",
    "            return None\n",
    "        elif e.e.code == 404:\n",
    "            print('Encountered 404 Error (Not Found)', file=sys.stderr)\n",
    "            return None\n",
    "        elif e.e.code == 429:\n",
    "            print('Encountered 429 Error (Rate Limit Exceeded)', file=sys.stderr)\n",
    "            if sleep_when_rate_limited:\n",
    "                print(\"Retrying in 15 minutes...ZzZ...\", file=sys.stderr)\n",
    "                sys.stderr.flush()\n",
    "                time.sleep(60*15 + 5)\n",
    "                print('...ZzZ...Awake now and trying again.', file=sys.stderr)\n",
    "                return 2\n",
    "            else:\n",
    "                raise e # Caller must handle the rate limiting issue\n",
    "        elif e.e.code in (500, 502, 503, 504):\n",
    "            print('Encountered %i Error. Retrying in %i seconds' % (e.e.code, wait_period), file=sys.stderr)\n",
    "            time.sleep(wait_period)\n",
    "            wait_period *= 1.5\n",
    "            return wait_period\n",
    "        else:\n",
    "            raise e\n",
    "\n",
    "    # End of nested helper function\n",
    "\n",
    "    wait_period = 2\n",
    "    error_count = 0\n",
    "    while True:\n",
    "        try:\n",
    "            return twitter_api_func(*args, **kw)\n",
    "        except TwitterHTTPError as e:\n",
    "            error_count = 0\n",
    "            wait_period = handle_twitter_http_error(e, wait_period)\n",
    "            if wait_period is None:\n",
    "                return\n",
    "        except URLError as e:\n",
    "            error_count += 1\n",
    "            print(\"URLError encountered. Continuing.\", file=sys.stderr)\n",
    "            if error_count > max_errors:\n",
    "                print(\"Too many consecutive errors...bailing out.\", file=sys.stderr)\n",
    "                raise\n",
    "        except BadStatusLine as e:\n",
    "            error_count += 1\n",
    "            print >> sys.stderr, \"BadStatusLine encountered. Continuing.\"\n",
    "            if error_count > max_errors:\n",
    "                print(\"Too many consecutive errors...bailing out.\", file=sys.stderr)\n",
    "                raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = make_twitter_request(t.users.lookup, screen_name=\"SocialWebMining\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will want to write responses to disk, on the fly, so that we can collect many observations for later analysis. In *Mining the Social Web*, the Mongo DB database program is recommended. Here is another way (that also can be adapted so that it writes to a database)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's wrap the extraction of tweet \"[entities](https://developer.twitter.com/en/docs/tweets/data-dictionary/overview/entities-object)\" in a function which takes a list of statuses as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tweet_entities(statuses):\n",
    "    if len(statuses) == 0:\n",
    "        return [], [], [], [], []\n",
    "    screen_names = [ user_mention['screen_name'] \n",
    "                    for status in statuses\n",
    "                        for user_mention in status['entities']['user_mentions'] ]\n",
    "    hashtags = [ hashtag['text']\n",
    "                    for status in statuses\n",
    "                        for hashtag in status['entities']['hashtags'] ]\n",
    "    urls = [ url['expanded_url']\n",
    "                    for status in statuses\n",
    "                        for url in status['entities']['urls'] ]\n",
    "    symbols = [ symbol['text']\n",
    "                    for status in statuses\n",
    "                        for symbol in status['entities']['symbols'] ]\n",
    "    if status['entities'].has_key('media'):\n",
    "        media = [ media['url']\n",
    "            for status in statuses\n",
    "                for media in status['entities']['media'] ]\n",
    "    else:\n",
    "        media = []\n",
    "    return screen_names, hashtags, urls, media, symbols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above function is a good template of sorts. You can use it to develop your own function for extracting other information from a *list of statuses*. \n",
    "\n",
    "Let's define two more functions, then we'll sample from the Twitter stream, writing results as we go. The functions below will take statuses, extract some information, and then write the results into a CSV file using Pandas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tweet_basics(status):\n",
    "    screen_name = None\n",
    "    tweet_ID = None\n",
    "    text = None\n",
    "    if 'user' in status:\n",
    "        screen_name = status['user']['screen_name'] \n",
    "        tweet_ID = status['id']\n",
    "        text = status['text']\n",
    "    return screen_name, tweet_ID, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_to_csv(file, status):\n",
    "    screen_name, tweet_ID, text = extract_tweet_basics(status)\n",
    "    df = pd.DataFrame([[screen_name,tweet_ID,text]], columns=['screen_name','tweet_ID','text'])\n",
    "    with open(file, 'a') as f:\n",
    "        df.to_csv(f,header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we stick this in the streaming loop from last time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a *streaming* connection (not RESTful, different from Search).\n",
    "t_stream = TwitterStream(auth=auth)\n",
    "\n",
    "\n",
    "# Get an *iterator* object from the twitter wrapper\n",
    "\n",
    "tweeterator = t_stream.statuses.sample()\n",
    "\n",
    "# Create a CSV file with column names\n",
    "# but no data (yet).\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(columns=['screen_name','tweet_ID','text'])\n",
    "df.to_csv('my_csv.csv', index=False)\n",
    "\n",
    "\n",
    "# The loop below will grab a new tweet,\n",
    "# extract some basic info, put that info\n",
    "# in a dataframe object, then use that\n",
    "# dataframe object to append one row to\n",
    "# the existing CSV file, 'my_csv.csv'.\n",
    "\n",
    "tweet_count = 100\n",
    "for tweet in tweeterator:\n",
    "    tweet_count -= 1\n",
    "    tweet_to_csv('my_csv.csv', tweet)  \n",
    "    if tweet_count <= 0:\n",
    "        break "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Network Structure**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we'll develop some tools for crawling the friendship graph of some Twitter followers. This exercise is taken directly from *Mining the Social Web*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will let us create new partial\n",
    "# functions with arguments set to \n",
    "# certain values.\n",
    "from functools import partial\n",
    "\n",
    "# This was maxint.\n",
    "# There is no longer a maxint (in Python 3)\n",
    "from sys import maxsize\n",
    "\n",
    "\n",
    "def get_friends_followers_ids(twitter_api, screen_name=None, user_id=None,\n",
    "                                friends_limit=maxsize, followers_limit=maxsize):\n",
    "    # Must have either screen_name or user_id (logical xor)\n",
    "    assert (screen_name != None) != (user_id != None), \\\n",
    "    \"Must have screen_name or user_id, but not both\"\n",
    "    \n",
    "    # You can also do this with a function closure.\n",
    "    get_friends_ids = partial(make_twitter_request, twitter_api.friends.ids,\n",
    "                                count=5000)\n",
    "    get_followers_ids = partial(make_twitter_request, twitter_api.followers.ids,\n",
    "                                count=5000)\n",
    "    friends_ids, followers_ids = [], []\n",
    "    for twitter_api_func, limit, ids, label in [\n",
    "            [get_friends_ids, friends_limit, friends_ids, \"friends\"],\n",
    "            [get_followers_ids, followers_limit, followers_ids, \"followers\"]\n",
    "            ]:\n",
    "        #LOOK HERE! This little line is important.\n",
    "        if limit == 0: continue\n",
    "        cursor = -1\n",
    "        while cursor != 0:\n",
    "            # Use make_twitter_request via the partially bound callable...\n",
    "            if screen_name:\n",
    "                response = twitter_api_func(screen_name=screen_name, cursor=cursor)\n",
    "            else: # user_id\n",
    "                response = twitter_api_func(user_id=user_id, cursor=cursor)\n",
    "            if response is not None:\n",
    "                ids += response['ids']\n",
    "                cursor = response['next_cursor']\n",
    "            print('Fetched {0} total {1} ids for {2}'.format(len(ids),\n",
    "                    label, (user_id or screen_name), file=sys.stderr))\n",
    "            if len(ids) >= limit or response is None:\n",
    "                break\n",
    "    # Do something useful with the IDs, like store them to disk...\n",
    "    return friends_ids[:friends_limit], followers_ids[:followers_limit]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "friends_ids, followers_ids = get_friends_followers_ids(t,\n",
    "                                screen_name=\"ZedShaw\",\n",
    "                                friends_limit=10,\n",
    "                                followers_limit=10)\n",
    "print(friends_ids)\n",
    "print(followers_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mostly empty data frame,\n",
    "# and write it to a CSV file.\n",
    "df = pd.DataFrame(columns=['ID','followers'])\n",
    "df.to_csv('followers.csv', index=False)\n",
    "\n",
    "# Our function\n",
    "def save_followers(fid, followers):\n",
    "    df = pd.DataFrame([[fid, followers]], columns=['ID','followers'])\n",
    "    with open('followers.csv', 'a') as f:\n",
    "        df.to_csv(f,header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_followers(twitter_api, screen_name, limit=1000000, depth=2):\n",
    "    \n",
    "    # Resolve the ID for screen_name and start working with IDs for consistency\n",
    "    seed_id = str(twitter_api.users.show(screen_name=screen_name)['id'])\n",
    "    _, next_queue = get_friends_followers_ids(twitter_api, user_id=seed_id,\n",
    "                        friends_limit=0, followers_limit=limit)\n",
    "    \n",
    "    # Store a seed_id => _follower_ids mapping in MongoDB\n",
    "    save_followers(seed_id, ','.join([str(x) for x in next_queue]))\n",
    "    \n",
    "    d = 1\n",
    "    # Note that in the example in the next cell,\n",
    "    # we never enter this loop.\n",
    "    while d < depth:\n",
    "        d += 1\n",
    "        # Reset the next_queue so that we can\n",
    "        # start building up the next level\n",
    "        # of followers-of-followers\n",
    "        (queue, next_queue) = (next_queue, [])\n",
    "        # Loop through the current\n",
    "        # level of followers\n",
    "        for fid in queue:\n",
    "            _, follower_ids = get_friends_followers_ids(twitter_api, user_id=fid,\n",
    "                                friends_limit=0, followers_limit=limit)\n",
    "            # Store an ID with a string recording\n",
    "            # IDs of followers of the user with ID \"fid\"\n",
    "            save_followers(str(fid), ','.join([str(x) for x in follower_ids]))\n",
    "            # Extending the list\n",
    "            next_queue += follower_ids\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
