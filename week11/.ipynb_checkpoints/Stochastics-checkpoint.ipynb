{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CPS600 - Python Programming for Finance \n",
    "###  \n",
    "<img src=\"https://www.syracuse.edu/wp-content/themes/g6-carbon/img/syracuse-university-seal.svg?ver=6.3.9\" style=\"width: 200px;\"/>\n",
    "\n",
    "## Stochastic Processes\n",
    "\n",
    "###  November 6, 2018\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will look this week at some useful statistical tools:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Normality tests\n",
    "* Portfolio theory\n",
    "* Principal component analysis\n",
    "* Bayesian regression\n",
    "* Hypothesis Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But before we get into that, let's look at *stochastic processes* from Chapter $10$ of the Hilpisch book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stochastic Processes**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the text:\n",
    "> Stochastics is one of the most important mathematical and numerical disciplines in finance.\n",
    "\n",
    "Then let's treat it with special attention! The main concepts are\n",
    "\n",
    "* Random Number Generation\n",
    "* Simulation\n",
    "* Valuation\n",
    "* Risk Measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Number Generation**\n",
    "\n",
    "Many tools are available for this task in Python. We'll use `numpy`, as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we've seen, we can generate a sample from the half-open interval $[0,1)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npr.rand(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also fill up an array with random values by giving a shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = npr.rand(5,5)\n",
    "M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a neat trick we saw the other day; you can sample from any kind of interval you want:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 10.\n",
    "b = 5.\n",
    "npr.rand(10)*(b-a) + a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are using broadcasting to do the same with a 2D array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npr.rand(5,5)*(b-a) + a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also saw how to use the uniform random number generator to sample from other distributions. For example the normal:\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/a/a9/Empirical_Rule.PNG\" style=\"width: 400px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do this by plugging our uniform sample into the inverse CDF or *quantile function*, also called the *percent point function* or *ppf*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "X = npr.rand(10000)\n",
    "Y = norm.ppf(X)\n",
    "plt.hist(Y, bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some other types of samples provided by `npr`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 500\n",
    "rn1 = npr.rand(sample_size, 3) # Many observations of 3 columns\n",
    "rn2 = npr.randint(0, 10, sample_size) # Includes 0, not 10. With replacement (obviously)\n",
    "rn3 = npr.sample(size=sample_size) # Random floats in [0.0,1.0)\n",
    "a = [0, 25, 50, 75, 100]\n",
    "rn4 = npr.choice(a, size=sample_size, replace=True) # Sample from a given array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a nice depiction of all of these together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(nrows=2, ncols=2,figsize=(7,7))\n",
    "\n",
    "ax1.hist(rn1, bins=25, stacked=True)\n",
    "ax1.set_title('rand')\n",
    "ax1.set_ylabel('frequency')\n",
    "ax1.grid(True)\n",
    "ax2.hist(rn2,bins=25)\n",
    "ax2.set_title('randint')\n",
    "ax2.grid(True)\n",
    "ax3.hist(rn3,bins=25)\n",
    "ax3.set_title('sample')\n",
    "ax3.set_ylabel('frequency')\n",
    "ax3.grid(True)\n",
    "ax4.hist(rn4,bins=25)\n",
    "ax4.set_title('choice')\n",
    "ax4.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is nice to know the *quantile function* trick above because it illustrates the relationship between CDF and sampling, but we need not do that most of the time because we have plenty of distributions implemented already in `numpy`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at some of these starting with the *standard normal* or *Gaussian*. From the text:\n",
    ">Although\tthere\tis\tmuch\tcriticism\taround\tthe\tuse\tof\t(standard)\tnormal\tdistributions\tin finance,\tthey\tare\tan\tindispensible\ttool\tand\tstill\tthe\tmost\twidely\tused\ttype\tof\tdistribution,\n",
    "in\tanalytical\tas\twell\tas\tnumerical\tapplications.\tOne\treason\tis\tthat\tmany\tfinancial\tmodels\n",
    "directly\trest\tin\tone\tway\tor\tanother\ton\ta\tnormal\tdistribution\tor\ta\tlog-normal\tdistribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 500\n",
    "rn1 = npr.standard_normal(sample_size)\n",
    "rn2 = npr.normal(100, 20, sample_size)\n",
    "rn3 = npr.chisquare(df=0.5, size=sample_size)\n",
    "rn4 = npr.poisson(lam=1.0, size=sample_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another nifty four-pane plot of these samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(nrows=2, ncols=2,figsize=(7,7))\n",
    "\n",
    "ax1.hist(rn1, bins=25)\n",
    "ax1.set_title('standard normal')\n",
    "ax1.set_ylabel('frequency')\n",
    "ax1.grid(True)\n",
    "ax2.hist(rn2,bins=25)\n",
    "ax2.set_title('normal (100,20)')\n",
    "ax2.grid(True)\n",
    "ax3.hist(rn3,bins=25)\n",
    "ax3.set_title('chi square')\n",
    "ax3.set_ylabel('frequency')\n",
    "ax3.grid(True)\n",
    "ax4.hist(rn4,bins=25)\n",
    "ax4.set_title('Poisson')\n",
    "ax4.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Simulation**\n",
    "\n",
    "What we just saw was the mathematical engine inside the stochastic financial models we really care about. *Monte Carlo* simulation is the modeling technique we'll discuss now.\n",
    "\n",
    "Again quoting your text:\n",
    ">Monte\tCarlo\tsimulation\t(MCS)\tis\tamong\tthe\tmost\timportant\tnumerical\ttechniques\tin\n",
    "finance,\tif\tnot\tthe\tmost\timportant\tand\twidely\tused."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But where does this odd name come from?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/f/f3/Le_casino_de_Monte-Carlo.JPG\" style=\"width: 600px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the *Black-Scholes-Merton* equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ S_T = S_0\\exp((r-\\frac{1}{2}\\sigma^2)T + \\sigma\\sqrt{T}z)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where\n",
    "\n",
    "* $S_T$ is the index level at time $T$.\n",
    "* $r$ is the constant riskless short rate.\n",
    "* $\\sigma$ is the constant volatility (= standard deviation of returns) of $S$.\n",
    "* $z$ is a *standard normally distributed* random variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set up the simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "S0 = 100 # Initial value\n",
    "r = 0.05 # Constant Short Rate\n",
    "sigma = 0.25 # Volatility\n",
    "T = 2.0 # in years\n",
    "I = 10000 # number of random draws\n",
    "z = npr.standard_normal(I)\n",
    "ST1 = S0*np.exp((r-.5*sigma**2)*T + sigma*np.sqrt(T)*z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we visualize this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(ST1, bins=50)\n",
    "plt.xlabel('index level')\n",
    "plt.ylabel('frequency')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should evoke feelings of *log-normality*, i.e. the log of this random variable is normally distributed. Therefore, we can also sample direction from the lognormal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "ST2 = S0 * npr.lognormal((r-.5*sigma**2)*T, sigma * np.sqrt(T), size=I)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(ST2, bins=50)\n",
    "plt.xlabel('index level')\n",
    "plt.ylabel('frequency')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let's compare numerically..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as scs\n",
    "\n",
    "def print_statistics(a1,a2):\n",
    "    sta1=scs.describe(a1)\n",
    "    sta2=scs.describe(a2)\n",
    "    print(\"%14s %14s %14s\" % ('statistic','data set 1', 'data set 2') )\n",
    "    print(45*\"\")\n",
    "    print(\"%14s %14.3f %14.3f\" % ('size', sta1[0], sta2[0]))\n",
    "    print(\"%14s %14.3f %14.3f\" % ('min', sta1[1][0], sta2[1][0]))\n",
    "    print(\"%14s %14.3f %14.3f\" % ('max', sta1[1][1], sta2[1][1]))\n",
    "    print(\"%14s %14.3f %14.3f\" % ('mean', sta1[2], sta2[2]))\n",
    "    print(\"%14s %14.3f %14.3f\" % ('std', np.sqrt(sta1[3]), np.sqrt(sta2[3])))\n",
    "    print(\"%14s %14.3f %14.3f\" % ('skew', sta1[4], sta2[4]))\n",
    "    print(\"%14s %14.3f %14.3f\" % ('kurtosis', sta1[5], sta2[5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_statistics(ST1,ST2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty close!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stochastic Processes**\n",
    "\n",
    "Let's talk about the *processes* that generate these data. Technically speaking, a *Stochastic process* is a sequence of random variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In finance, stochastic processes generally exhibit the *Markov property*:\n",
    "\n",
    ">that\ttomorrow’s\tvalue\tof\tthe\tprocess\tonly\tdepends\ton\ttoday’s\tstate\tof\n",
    "the\tprocess,\tand\tnot\tany\tother\tmore\t“historic”\tstate\tor\teven\tthe\twhole\tpath\thistory.\tThe\n",
    "process\tthen\tis\talso\tcalled\tmemoryless."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Geometric Brownian Motion**\n",
    "\n",
    "Consider the BSM model in its differential form:\n",
    "\n",
    "$$ S_t = S_{t-\\Delta t} \\exp((r-\\frac{1}{2}\\sigma^2)\\Delta t + \\sigma\\sqrt{\\Delta t}z_t) $$\n",
    "\n",
    "We translate into Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "I = 10000\n",
    "M = 50\n",
    "dt = T / M\n",
    "S = np.zeros((M+1,I))\n",
    "S[0] = S0\n",
    "for t in range(1,M+1):\n",
    "    S[t] = S[t-1]*np.exp((r - .5*sigma**2)*dt\n",
    "                         + sigma * np.sqrt(dt) * npr.standard_normal(I))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again our data have a lognormal distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(ST[-1], bins=50)\n",
    "plt.xlabel('index level')\n",
    "plt.ylabel('frequency')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we can compare the summary statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_statistics(S[-1],ST2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can visualize the first 10 *simulated paths* (stop and think about what that really means)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(S[:,:10],lw=1.5)\n",
    "plt.xlabel('time')\n",
    "plt.ylabel('index_level')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's enough of that for now. We'll pick with the *square-root diffusion* model next time (an example of a mean-reverting process)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Statistics**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Normality Tests**\n",
    "\n",
    "Important financial models rest on the assumption that returns of securities are normally distributed - but this assumption is not always valid.\n",
    "\n",
    "Let's discuss approaches to test a given time series for normality of returns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Benchmark Case**\n",
    "\n",
    "We will take another look at *paths* just like the ones we saw above. Let's encapsulate it in a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_paths(S0, r, sigma, T, M, I):\n",
    "    ''' Generate Monte Carlo paths for geometric Brownian motion.\n",
    "    \n",
    "    Parameters\n",
    "    ==========\n",
    "    S0 : float      initial stock/index value\n",
    "    r : float       constant short rate\n",
    "    sigma : float   constant volatility\n",
    "    T : float       final time horizon\n",
    "    M : int         number of time steps/intervals\n",
    "    I : int         number of paths to be simulated\n",
    "        \n",
    "    Returns\n",
    "    =======\n",
    "    paths : ndarray, shape (M + 1, I)\n",
    "        simulated paths given the parameters\n",
    "    '''\n",
    "    dt = float(T) / M\n",
    "    paths = np.zeros((M + 1, I), np.float64)\n",
    "    paths[0] = S0\n",
    "    for t in range(1, M + 1):\n",
    "        rand = np.random.standard_normal(I)\n",
    "        rand = (rand - rand.mean()) / rand.std()\n",
    "        paths[t] = paths[t - 1] * np.exp((r - 0.5 * sigma ** 2) * dt +\n",
    "                                         sigma * np.sqrt(dt) * rand)\n",
    "    return paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the parameters for a Monte Carlo simulation, generating via the function `gen_paths` $250,000$ paths with $50$ time steps each:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "S0 = 100.\n",
    "r = 0.05\n",
    "sigma = 0.2\n",
    "T = 1.0\n",
    "M = 50\n",
    "I = 250000\n",
    "paths = gen_paths(S0, r, sigma, T, M, I)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot again; $10$ simulated paths (out of hundreds of thousands) of geometric Brownian motion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(paths[:, :10])\n",
    "plt.grid(True)\n",
    "plt.xlabel('time steps')\n",
    "plt.ylabel('index level')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we generate an `ndarray` object with all log returns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_returns = np.log(paths[1:] / paths[0:-1]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is the first path over all 50 time steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths[:, 0].round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the log-return series for a simulated path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_returns[:, 0].round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a different function for printing statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printStatistics(array):\n",
    "    ''' Prints selected statistics.\n",
    "    \n",
    "    Parameters\n",
    "    ==========\n",
    "    array: ndarray\n",
    "        object to generate statistics on\n",
    "    '''\n",
    "    sta = scs.describe(array)\n",
    "    print (\"%14s %15s\" % ('statistic', 'value'))\n",
    "    print (30 * \"-\")\n",
    "    print (\"%14s %15.5f\" % ('size', sta[0]))\n",
    "    print (\"%14s %15.5f\" % ('min', sta[1][0]))\n",
    "    print (\"%14s %15.5f\" % ('max', sta[1][1]))\n",
    "    print (\"%14s %15.5f\" % ('mean', sta[2]))\n",
    "    print (\"%14s %15.5f\" % ('std', np.sqrt(sta[3])))\n",
    "    print (\"%14s %15.5f\" % ('skew', sta[4]))\n",
    "    print (\"%14s %15.5f\" % ('kurtosis', sta[5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we flatten the 2D array and print a summary of all of the log-return data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_statistics(log_returns.flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to test normality is by visual inspection..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(log_returns.flatten(), bins=70, density=True)\n",
    "plt.grid(True)\n",
    "plt.xlabel('log-return')\n",
    "plt.ylabel('frequency')\n",
    "x = np.linspace(plt.axis()[0], plt.axis()[1])\n",
    "plt.plot(x, scs.norm.pdf(x, loc=r / M, scale=sigma / np.sqrt(M)),\n",
    "         'r', lw=2.0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to graphically 'test' for normality is to create quantile-quantile plot. Here, sample quantile values are compared to theoretical quantile values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.qqplot(log_returns.flatten()[::500], line='s')\n",
    "plt.grid(True)\n",
    "plt.xlabel('theoretical quantiles')\n",
    "plt.ylabel('sample quantiles')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a (slightly) more rigorous analysis of normality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normality_tests(array):\n",
    "    ''' Tests for normality distribution of given data set.\n",
    "    \n",
    "    Parameters\n",
    "    ==========\n",
    "    array: ndarray\n",
    "        object to generate statistics on\n",
    "    '''\n",
    "    print (\"Skew of data set  %14.3f\" % scs.skew(array))\n",
    "    print (\"Skew test p-value %14.3f\" % scs.skewtest(array)[1])\n",
    "    print (\"Kurt of data set  %14.3f\" % scs.kurtosis(array))\n",
    "    print (\"Kurt test p-value %14.3f\" % scs.kurtosistest(array)[1])\n",
    "    print (\"Norm test p-value %14.3f\" % scs.normaltest(array)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normality_tests(log_returns.flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally, a *p-value* is the probability of *falsely rejecting the null hypothesis given that the null hypothesis is true*. So we want these p-values to be high. Two of them look pretty high, though it'd be nice if that last guy were higher."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was a look at all of the log-return values. Now we will check whether the end-of-period values are log-normal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(9, 4))\n",
    "ax1.hist(paths[-1], bins=30)\n",
    "ax1.grid(True)\n",
    "ax1.set_xlabel('index level')\n",
    "ax1.set_ylabel('frequency')\n",
    "ax1.set_title('regular data')\n",
    "ax2.hist(np.log(paths[-1]), bins=30)\n",
    "ax2.grid(True)\n",
    "ax2.set_xlabel('log index level')\n",
    "ax2.set_title('log data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks pretty normal, if you ask me."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The statistics for the data set show expected behavior — for example, a mean value close to $105$ and a standard deviation (volatility) close to $20\\%$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printStatistics(paths[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean and volatility are close to what we'd expect based on the parameters $r=.05$ and $\\sigma = .2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The log index level values also have skew and kurtosis values close to zero:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printStatistics(np.log(paths[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data set also shows high p-values, providing strong support for the normal distribution hypothesis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normality_tests(np.log(paths[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, visual inspection of the match between our distribution and a normal directly sampled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_data = np.log(paths[-1])\n",
    "plt.hist(log_data, bins=70, density=True, label='observed')\n",
    "plt.grid(True)\n",
    "plt.xlabel('index levels')\n",
    "plt.ylabel('frequency')\n",
    "x = np.linspace(plt.axis()[0], plt.axis()[1])\n",
    "plt.plot(x, scs.norm.pdf(x, log_data.mean(), log_data.std()),\n",
    "         'r', lw=2.0, label='pdf')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, again, the quantile-quantile plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.qqplot(log_data, line='s')\n",
    "plt.grid(True)\n",
    "plt.xlabel('theoretical quantiles')\n",
    "plt.ylabel('sample quantiles')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Real World Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from iexfinance import get_historical_data\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to analyze four historical time series: two tracking ETFs (the SPY and DIA) and two stocks (Apple and Google).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbols = ['SPY', 'DIA', 'AAPL', 'GOOG']\n",
    "\n",
    "# Let's start four years ago\n",
    "start = datetime(2014, 11, 6)\n",
    "\n",
    "# Here is our query.\n",
    "data = get_historical_data(symbols=symbols, start=start,  output_format='pandas')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must convert this to a dataframe - the 'output_format' parameter is not enough because of what `data` actually is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['SPY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = pd.DataFrame({c:data[c]['close'] for c in symbols}, columns=symbols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's have a look at it.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And what is in it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get the four time series normalized to a starting value of 100:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(frame / frame.iloc[0] * 100).plot(figsize=(8, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, we can calculate the log-returns using the shift method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_returns = np.log(frame / frame.shift(1))\n",
    "log_returns.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we have a look.\n",
    "\n",
    "We guess that these distributions are not normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_returns.hist(bins=50, figsize=(9, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's ask our statistical methods..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sym in symbols:\n",
    "    print (\"\\nResults for symbol %s\" % sym)\n",
    "    print (30 * \"-\")\n",
    "    log_data = np.array(log_returns[sym].dropna())\n",
    "    printStatistics(log_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The observation here is that the kurtosis is too far off for normality. Intuitively, you should think of it as *peakedness* - look again at the picture!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also have the quantile-quantile plot tool at our disposal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.qqplot(log_returns['SPY'].dropna(), line='s')\n",
    "plt.grid(True)\n",
    "plt.xlabel('theoretical quantiles')\n",
    "plt.ylabel('sample quantiles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.qqplot(log_returns['DIA'].dropna(), line='s')\n",
    "plt.grid(True)\n",
    "plt.xlabel('theoretical quantiles')\n",
    "plt.ylabel('sample quantiles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.qqplot(log_returns['AAPL'].dropna(), line='s')\n",
    "plt.grid(True)\n",
    "plt.xlabel('theoretical quantiles')\n",
    "plt.ylabel('sample quantiles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.qqplot(log_returns['GOOG'].dropna(), line='s')\n",
    "plt.grid(True)\n",
    "plt.xlabel('theoretical quantiles')\n",
    "plt.ylabel('sample quantiles')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sample quantile values do not lie on a straight line, indicating non-normality. The time series data exhibits fat tails."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At long last, we go ahead and do the formal normality tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sym in symbols:\n",
    "    print (\"\\nResults for symbol %s\" % sym)\n",
    "    print (32 * \"-\")\n",
    "    log_data = np.array(log_returns[sym].dropna())\n",
    "    normality_tests(log_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tiny p-values (effectively zero), means that there is no chance of falsely rejecting the hypothesis of normality. Therefore we reject it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Portfolio Optimization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following example is based on MPT - *mean-variance portfolio theory*. The basic idea of MPT is diversification to achieve a minimal portfolio risk or maximal portfolio returns given a certain level of risk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with another set of stocks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New stocks\n",
    "symbols = ['AAPL', 'GOOG', 'MSFT', 'DB', 'GLD']\n",
    "\n",
    "noa = len(symbols)\n",
    "\n",
    "# Let's start four years ago\n",
    "start = datetime(2014, 11, 6)\n",
    "\n",
    "# Here is our query.\n",
    "data = get_historical_data(symbols=symbols, start=start,  output_format='pandas')\n",
    "\n",
    "# Getting just the close values.\n",
    "frame = pd.DataFrame({c:data[c]['close'] for c in symbols}, columns=symbols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, taking a normalized look at it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(frame / frame.iloc[0] * 100).plot(figsize=(8, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *mean-variance* in the name for this approach refers to the mean variance of the log-returns of the various securities, calculated thus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "rets = np.log(frame / frame.shift(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a factor of $252$ trading days to annualize the daily returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rets.mean() * 252"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get the *covariance matrix*, which says something about how the different stocks vary together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rets.cov() * 252"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Basic Theory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.random.random(noa)\n",
    "weights /= np.sum(weights)\n",
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### General formula for expected portfolio return\n",
    "\n",
    "$\\Large \\mu_p = E(\\displaystyle\\sum_I w_ir_i) = \\sum_I w_i E(r_i) = \\sum_I w_i \\mu_i = w^T \\mu$\n",
    "\n",
    "$I$ is the number of assets \n",
    "\n",
    "$w_i$  is the weight of asset $i$ ($w_i ≥ 0$, $\\sum_I w_i = 1$)\n",
    "\n",
    "$r_i$ are the state-dependent future returns\n",
    "\n",
    "$\\mu_i$ is the expected return for security $i$\n",
    "\n",
    "$w_T$ is the transpose of the weights vector \n",
    "\n",
    "$\\mu$  is the vector of the expected security returns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this is another *linear model*. We saw some of those before. Note that there is nothing really profound here; we're just weighting the different returns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expected portfolio return, we multiply again by $252$ to get annualized return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(rets.mean() * weights) * 252"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It gets a little more interesting when we consider the volatility of our portfolio. For that we'll need the covariance matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Portfolio covariance matrix\n",
    "\n",
    "$\\Large \\sigma_{ij} = \\sigma_{ji} = E((r_i - \\mu_i)(r_j - \\mu_j))$\n",
    "\n",
    "##### variance of a security is a special case of the covariance\n",
    "\n",
    "$\\Large \\sigma_{i}^2 =  E((r_i - \\mu_i)^2)$\n",
    "\n",
    "##### General formula for expected portfolio variance\n",
    "\n",
    "$\\Large \\sigma_{p}^2 =  E((r - \\mu)^2)  = \\displaystyle\\sum_{i \\in I}\\sum_{j \\in I} w_i w_j \\sigma_{ij} = w^T \\sum w$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dot function gives the dot product of two vectors/matrices. The T or transpose method gives the transpose of a vector or matrix. We can use these to calculate the covariance and the standard deviation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.dot(weights.T, np.dot(rets.cov() * 252, weights)),\n",
    " np.sqrt(np.dot(weights.T, np.dot(rets.cov() * 252, weights))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's implement a Monte Carlo simulation to generate random portfolio weight vectors on a larger scale. For every simulated allocation, we record the resulting expected portfolio return and variance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "prets = []\n",
    "pvols = []\n",
    "for p in range (2500):\n",
    "    weights = np.random.random(noa)\n",
    "    weights /= np.sum(weights)\n",
    "    prets.append(np.sum(rets.mean() * weights) * 252)\n",
    "    pvols.append(np.sqrt(np.dot(weights.T, \n",
    "                        np.dot(rets.cov() * 252, weights))))\n",
    "prets = np.array(prets)\n",
    "pvols = np.array(pvols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "plt.scatter(pvols, prets, c=prets / pvols, marker='o')\n",
    "plt.grid(True)\n",
    "plt.xlabel('expected volatility')\n",
    "plt.ylabel('expected return')\n",
    "plt.colorbar(label='Sharpe ratio')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An investor  is generally interested in the maximum return given a fixed risk level or the minimum risk given a fixed return expectation. This set of portfolios then makes up the so-called efficient frontier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out the definition of the [Sharpe ratio](https://en.wikipedia.org/wiki/Sharpe_ratio). We want it to be high."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optimization**\n",
    "\n",
    "So far we just saw description and simulation - how do we figure out an optimal portfolio weighting, given some set of securities?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.optimize as sco # Time to optimize again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the code in the function below refers to the returns floating around in the main namespace - *you will have to do something slightly different in the lab*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def statistics(weights):\n",
    "    ''' Return portfolio statistics.\n",
    "    \n",
    "    Parameters\n",
    "    ==========\n",
    "    weights : array-like\n",
    "        weights for different securities in portfolio\n",
    "    \n",
    "    Returns\n",
    "    =======\n",
    "    pret : float\n",
    "        expected portfolio return\n",
    "    pvol : float\n",
    "        expected portfolio volatility\n",
    "    pret / pvol : float\n",
    "        Sharpe ratio for rf=0\n",
    "    '''\n",
    "    weights = np.array(weights)\n",
    "    pret = np.sum(rets.mean() * weights) * 252\n",
    "    pvol = np.sqrt(np.dot(weights.T, np.dot(rets.cov() * 252, weights)))\n",
    "    return np.array([pret, pvol, pret / pvol])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The derivation of the optimal portfolios is a constrained optimization problem for which we use the function minimize from the `scipy.optimize` sublibrary.\n",
    "\n",
    "Let us start with the maximization of the Sharpe ratio. Formally, we minimize the negative value of the Sharpe ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_func_sharpe(weights):\n",
    "    return -statistics(weights)[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall how we provide constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "cons = ({'type': 'eq', 'fun': lambda x:  np.sum(x) - 1})\n",
    "bnds = tuple((0, 1) for x in range(noa))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we do the optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "opts = sco.minimize(min_func_sharpe, noa * [1. / noa,], method='SLSQP',\n",
    "                       bounds=bnds, constraints=cons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the results. We access the results object by providing the key of interest \n",
    " — i.e., x in our case.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opts['x'].round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expected return, volatility and sharpe ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics(opts['x']).round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's minimize the variance of the portfolio. Our function describing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_func_variance(weights):\n",
    "    return statistics(weights)[1] ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The actual optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "optv = sco.minimize(min_func_variance, noa * [1. / noa,], method='SLSQP',\n",
    "                       bounds=bnds, constraints=cons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optv['x'].round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics(optv['x']).round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Efficient Frontier**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we look at how to calculate the efficient frontier. Remember this means we maximize return subject to a given risk or we minimize risky subject to an expected return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "cons = ({'type': 'eq', 'fun': lambda x:  statistics(x)[0] - tret},\n",
    "        {'type': 'eq', 'fun': lambda x:  np.sum(x) - 1})\n",
    "bnds = tuple((0, 1) for x in weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_func_port(weights):\n",
    "    return statistics(weights)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quoting from the text:\n",
    ">The\tapproach\twe\ttake\tis\tthat\twe\tfix\ta\ttarget\treturn\tlevel\tand\n",
    "derive\tfor\teach\tsuch\tlevel\tthose\tportfolio\tweights\tthat\tlead\tto\tthe\tminimum\tvolatility\n",
    "value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "trets = np.linspace(0.0, 0.21, 50)\n",
    "tvols = []\n",
    "for tret in trets:\n",
    "    cons = ({'type': 'eq', 'fun': lambda x:  statistics(x)[0] - tret},\n",
    "            {'type': 'eq', 'fun': lambda x:  np.sum(x) - 1})\n",
    "    res = sco.minimize(min_func_port, noa * [1. / noa,], method='SLSQP',\n",
    "                       bounds=bnds, constraints=cons)\n",
    "    tvols.append(res['fun'])\n",
    "tvols = np.array(tvols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the results:\n",
    ">Crosses\tindicate\tthe\toptimal\tportfolios\tgiven\n",
    "a\tcertain\ttarget\treturn;\tthe\tdots\tare,\tas\tbefore,\tthe\trandom\tportfolios.\tIn\taddition,\tthe\tfigure\n",
    "shows\ttwo\tlarger\tstars:\tone\tfor\tthe\tminimum\tvolatility/variance\tportfolio\t(the\tleftmost\n",
    "portfolio)\tand\tone\tfor\tthe\tportfolio\twith\tthe\tmaximum\tSharpe\tratio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,4))\n",
    "plt.scatter(pvols,prets,c=prets/pvols,marker='o')#random portfolio composition\n",
    "plt.scatter(tvols,trets,c=trets/tvols,marker='x')#efficient frontier\n",
    "#portfolio with highest Sharpe ratio\n",
    "plt.plot(statistics(opts['x'])[1], statistics(opts['x'])[0],'r*',markersize=15.0)\n",
    "#minimum variance portfolio\n",
    "plt.plot(statistics(optv['x'])[1],statistics(optv['x'])[0],'y*',markersize=15.0)\n",
    "plt.grid(True)\n",
    "plt.xlabel('expected volatility')\n",
    "plt.ylabel('expected return')\n",
    "plt.colorbar(label='Sharpe ratio')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Capital Market Line**\n",
    "\n",
    "Let's finish off with the *Capital Market Line*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.interpolate as sci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = np.argmin(tvols)\n",
    "evols = tvols[ind:]\n",
    "erets = trets[ind:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpoloating..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "tck = sci.splrep(evols, erets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a function describing the frontier and also getting its derivative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return sci.splev(x, tck, der=0)\n",
    "def df(x):\n",
    "    return sci.splev(x, tck, der=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What we are looking for is a function t(x) = a + b · x describing the line that passes\n",
    "##### through the riskless asset in risk-return space and that is tangent to the efficient frontier.\n",
    "\n",
    "##### Mathematical conditions for capital market line\n",
    "\n",
    "$\\Large t(x) = a + b \\cdot x$\n",
    "\n",
    "$\\Large t(0) = r_f   \\Leftrightarrow     a = r_f$\n",
    "\n",
    "$\\Large t(x) = f(x)   \\Leftrightarrow   a + b \\cdot x = f(x)$ \n",
    "\n",
    "$\\Large t'(x) = f'(x)  \\Leftrightarrow   b = f'(x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def equations(p, rf=0.01):\n",
    "    eq1 = rf - p[0]\n",
    "    eq2 = rf + p[1] * p[2] - f(p[2])\n",
    "    eq3 = p[1] - df(p[2])\n",
    "    return eq1, eq2, eq3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solving...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = sco.fsolve(equations, [0.01, 0.1, 0.1])\n",
    "opt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the equations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.round(equations(opt), 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pvols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having a look at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "plt.scatter(pvols, prets,\n",
    "            c=(prets - 0.01) / pvols, marker='o')\n",
    "            # random portfolio composition\n",
    "plt.plot(evols, erets, 'g', lw=4.0)\n",
    "            # efficient frontier\n",
    "cx = np.linspace(0.0, 0.3)\n",
    "plt.plot(cx, opt[0] + opt[1] * cx, lw=1.5)\n",
    "            # capital market line\n",
    "plt.plot(opt[2], f(opt[2]), 'r*', markersize=15.0) \n",
    "plt.grid(True)\n",
    "plt.axhline(0, color='k', ls='--', lw=2.0)\n",
    "plt.axvline(0, color='k', ls='--', lw=2.0)\n",
    "plt.xlabel('expected volatility')\n",
    "plt.ylabel('expected return')\n",
    "plt.colorbar(label='Sharpe ratio')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beautiful and perfect! Finally, we are interested in the portfolio weights that actually did it for us. What are they?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "cons = ({'type': 'eq', 'fun': lambda x:  statistics(x)[0] - f(opt[2])},\n",
    "        {'type': 'eq', 'fun': lambda x:  np.sum(x) - 1})\n",
    "res = sco.minimize(min_func_port, noa * [1. / noa,], method='SLSQP',\n",
    "                       bounds=bnds, constraints=cons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res['x'].round(3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
